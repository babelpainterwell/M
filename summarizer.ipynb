{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.messages import AIMessage, BaseMessage, HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_fireworks import ChatFireworks\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are an deep learning model paper writer assistant tasked with writing excellent and comprehensive and detailed description to describe a deep learning project.\"\n",
    "            \" Generate the best paper possible given user's code, which is a deep learning project containing the code to train the model, the code of model architecture and the code for data preparation\"\n",
    "            \"To generate the best paper, you should consider the following points: parameter use, model architecture such as what does each layer do and what parameters are used, data preparation such as input data size or any preprocessing, training such as loss funcion or normalization, and evaluation such as what metrics being used, inference such as what post processing is done\"\n",
    "            \"You must make it clear what the code does, how it works, and why it is important. The summary should be detailed and comprehensive, but also concise and easy to understand. The summary should be written in a professional and engaging tone, and should be free of grammatical errors. Especially pay attention to parameter use in the model\"\n",
    "            \" If the user provides critique, respond with a revised version of your previous attempts.\"\n",
    "            f\"The overall structure can be referred to {demo}. You should not use any facts such as data from the demo that are not provided by the user in the code,\"\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    ")\n",
    "generate = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Centerline Detection of Neuronal Dendrites: Deep Learning Model Workflow\n",
      "\n",
      "## 1. Introduction\n",
      "\n",
      "The objective of this project is to develop a robust deep learning model to detect the centerlines of neuronal dendrite membranes from grayscale images. The main goal is to accurately identify the 'backbone' structure within the images, ensuring continuity even in the presence of noise. This task is categorized under binary segmentation, focusing on isolating central lines within membrane structures. The provided dataset consists of 16-bit grayscale images of size 65x65 pixels. The membrane structures have intensity values ranging from approximately 150 to 1000, while the background intensity values range from 110 to 150. Accurate segmentation is challenging due to noise, which sometimes has pixel values similar to those of the membranes.\n",
      "\n",
      "## 2. Data Preparation\n",
      "\n",
      "### 2.1 Data Collection\n",
      "The dataset comprises 28,634 samples stored as 65x65 16-bit PNG images. Each image is grayscale, with corresponding segmentation masks also stored as 65x65 PNGs. This ensures a direct mapping between images and their labels, facilitating the training and evaluation processes.\n",
      "\n",
      "### 2.2 Data Preprocessing\n",
      "Effective preprocessing is critical for preparing the data for model training. The following steps were undertaken:\n",
      "\n",
      "1. **Intensity Range Adjustment**: The intensity range for the background was adjusted to [100, 150] and for the membrane to [150, 1010]. This adjustment ensures a higher percentage of samples fall within these ranges, improving the model's robustness.\n",
      "\n",
      "2. **Handling Empty Samples**: Initially, 1,000 samples (3.5% of the dataset) contained no centerline information. These were reduced to 100 (0.35%) to help the model learn to identify the absence of centerlines without biasing it towards predicting no centerline.\n",
      "\n",
      "3. **Normalization and Clipping**: Images were clipped to the specified ranges and normalized to the [0, 1] range using MINMAX Normalization. This ensures consistent pixel values for model training, enhancing convergence.\n",
      "\n",
      "4. **Data Augmentation**: Applied transformations to increase the training data diversity:\n",
      "   - **Elastic Deformation**: Simulates realistic distortions in membrane images.\n",
      "   - **Horizontal and Vertical Flips**: Enhances the modelâ€™s ability to recognize membranes irrespective of their orientation.\n",
      "\n",
      "5. **Dataset and DataLoader Construction**: Custom PyTorch Dataset classes (CenterlineDataset) were implemented to handle preprocessing and facilitate data loading during training. DataLoader objects were created for batching and shuffling data during training and evaluation phases.\n",
      "\n",
      "### 2.3 Data Splitting\n",
      "The dataset was split into training (70%), validation (15%), and test sets (15%) to ensure"
     ]
    },
    {
     "ename": "RemoteProtocolError",
     "evalue": "peer closed connection without sending complete message body (incomplete chunked read)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpx/_transports/default.py:69\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 69\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpx/_transports/default.py:113\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mfor\u001b[39;00m part \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_httpcore_stream:\n\u001b[1;32m    114\u001b[0m         \u001b[39myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:367\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 367\u001b[0m \u001b[39mraise\u001b[39;00m exc \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:363\u001b[0m, in \u001b[0;36mPoolByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m     \u001b[39mfor\u001b[39;00m part \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream:\n\u001b[1;32m    364\u001b[0m         \u001b[39myield\u001b[39;00m part\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpcore/_sync/http11.py:349\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[0;32m--> 349\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpcore/_sync/http11.py:341\u001b[0m, in \u001b[0;36mHTTP11ConnectionByteStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mreceive_response_body\u001b[39m\u001b[39m\"\u001b[39m, logger, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request, kwargs):\n\u001b[0;32m--> 341\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39m_receive_response_body(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    342\u001b[0m         \u001b[39myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpcore/_sync/http11.py:210\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_body\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 210\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mData):\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpcore/_sync/http11.py:221\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions({h11\u001b[39m.\u001b[39mRemoteProtocolError: RemoteProtocolError}):\n\u001b[0;32m--> 221\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpcore/_exceptions.py:14\u001b[0m, in \u001b[0;36mmap_exceptions\u001b[0;34m(map)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(exc, from_exc):\n\u001b[0;32m---> 14\u001b[0m         \u001b[39mraise\u001b[39;00m to_exc(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m: peer closed connection without sending complete message body (incomplete chunked read)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/wellzhang/Desktop/Mortal/summarizer.ipynb Cell 2\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wellzhang/Desktop/Mortal/summarizer.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m summary \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wellzhang/Desktop/Mortal/summarizer.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m request \u001b[39m=\u001b[39m HumanMessage(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wellzhang/Desktop/Mortal/summarizer.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     content\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWrite a 3-page paper on the code provided, which is a segmentation model used to detect centerlines of dendrites under microscope\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mcode\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wellzhang/Desktop/Mortal/summarizer.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/wellzhang/Desktop/Mortal/summarizer.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m generate\u001b[39m.\u001b[39mstream({\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m: [request]}):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wellzhang/Desktop/Mortal/summarizer.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mprint\u001b[39m(chunk\u001b[39m.\u001b[39mcontent, end\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/wellzhang/Desktop/Mortal/summarizer.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     summary \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m chunk\u001b[39m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/langchain_core/runnables/base.py:2786\u001b[0m, in \u001b[0;36mRunnableSequence.stream\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2780\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstream\u001b[39m(\n\u001b[1;32m   2781\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2782\u001b[0m     \u001b[39minput\u001b[39m: Input,\n\u001b[1;32m   2783\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2784\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2785\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2786\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(\u001b[39miter\u001b[39m([\u001b[39minput\u001b[39m]), config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/langchain_core/runnables/base.py:2773\u001b[0m, in \u001b[0;36mRunnableSequence.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   2767\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\n\u001b[1;32m   2768\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   2769\u001b[0m     \u001b[39minput\u001b[39m: Iterator[Input],\n\u001b[1;32m   2770\u001b[0m     config: Optional[RunnableConfig] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   2771\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   2772\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[Output]:\n\u001b[0;32m-> 2773\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_stream_with_config(\n\u001b[1;32m   2774\u001b[0m         \u001b[39minput\u001b[39m,\n\u001b[1;32m   2775\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform,\n\u001b[1;32m   2776\u001b[0m         patch_config(config, run_name\u001b[39m=\u001b[39m(config \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_name\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname),\n\u001b[1;32m   2777\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2778\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/langchain_core/runnables/base.py:1778\u001b[0m, in \u001b[0;36mRunnable._transform_stream_with_config\u001b[0;34m(self, input, transformer, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1776\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1777\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m-> 1778\u001b[0m         chunk: Output \u001b[39m=\u001b[39m context\u001b[39m.\u001b[39;49mrun(\u001b[39mnext\u001b[39;49m, iterator)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1779\u001b[0m         \u001b[39myield\u001b[39;00m chunk\n\u001b[1;32m   1780\u001b[0m         \u001b[39mif\u001b[39;00m final_output_supported:\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/langchain_core/runnables/base.py:2735\u001b[0m, in \u001b[0;36mRunnableSequence._transform\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   2732\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2733\u001b[0m         final_pipeline \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39mtransform(final_pipeline, config)\n\u001b[0;32m-> 2735\u001b[0m \u001b[39mfor\u001b[39;00m output \u001b[39min\u001b[39;00m final_pipeline:\n\u001b[1;32m   2736\u001b[0m     \u001b[39myield\u001b[39;00m output\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/langchain_core/runnables/base.py:1172\u001b[0m, in \u001b[0;36mRunnable.transform\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   1169\u001b[0m             final \u001b[39m=\u001b[39m ichunk\n\u001b[1;32m   1171\u001b[0m \u001b[39mif\u001b[39;00m got_first_val:\n\u001b[0;32m-> 1172\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream(final, config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:265\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    259\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(\n\u001b[1;32m    260\u001b[0m         e,\n\u001b[1;32m    261\u001b[0m         response\u001b[39m=\u001b[39mLLMResult(\n\u001b[1;32m    262\u001b[0m             generations\u001b[39m=\u001b[39m[[generation]] \u001b[39mif\u001b[39;00m generation \u001b[39melse\u001b[39;00m []\n\u001b[1;32m    263\u001b[0m         ),\n\u001b[1;32m    264\u001b[0m     )\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    266\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_end(LLMResult(generations\u001b[39m=\u001b[39m[[generation]]))\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:245\u001b[0m, in \u001b[0;36mBaseChatModel.stream\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m generation: Optional[ChatGenerationChunk] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream(messages, stop\u001b[39m=\u001b[39mstop, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m         \u001b[39mif\u001b[39;00m chunk\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mid \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m             chunk\u001b[39m.\u001b[39mmessage\u001b[39m.\u001b[39mid \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrun-\u001b[39m\u001b[39m{\u001b[39;00mrun_manager\u001b[39m.\u001b[39mrun_id\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/langchain_openai/chat_models/base.py:483\u001b[0m, in \u001b[0;36mBaseChatOpenAI._stream\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m default_chunk_class \u001b[39m=\u001b[39m AIMessageChunk\n\u001b[1;32m    482\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(messages\u001b[39m=\u001b[39mmessage_dicts, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams) \u001b[39mas\u001b[39;00m response:\n\u001b[0;32m--> 483\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m response:\n\u001b[1;32m    484\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunk, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    485\u001b[0m             chunk \u001b[39m=\u001b[39m chunk\u001b[39m.\u001b[39mmodel_dump()\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/openai/_streaming.py:46\u001b[0m, in \u001b[0;36mStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[_T]:\n\u001b[0;32m---> 46\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator:\n\u001b[1;32m     47\u001b[0m         \u001b[39myield\u001b[39;00m item\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/openai/_streaming.py:58\u001b[0m, in \u001b[0;36mStream.__stream__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m process_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_client\u001b[39m.\u001b[39m_process_response_data\n\u001b[1;32m     56\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_events()\n\u001b[0;32m---> 58\u001b[0m \u001b[39mfor\u001b[39;00m sse \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m     59\u001b[0m     \u001b[39mif\u001b[39;00m sse\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m[DONE]\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     60\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/openai/_streaming.py:50\u001b[0m, in \u001b[0;36mStream._iter_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_iter_events\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[0;32m---> 50\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_decoder\u001b[39m.\u001b[39miter_bytes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse\u001b[39m.\u001b[39miter_bytes())\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/openai/_streaming.py:280\u001b[0m, in \u001b[0;36mSSEDecoder.iter_bytes\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39miter_bytes\u001b[39m(\u001b[39mself\u001b[39m, iterator: Iterator[\u001b[39mbytes\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Iterator[ServerSentEvent]:\n\u001b[1;32m    279\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Given an iterator that yields raw binary data, iterate over it & yield every event encountered\"\"\"\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iter_chunks(iterator):\n\u001b[1;32m    281\u001b[0m         \u001b[39m# Split before decoding so splitlines() only uses \\r and \\n\u001b[39;00m\n\u001b[1;32m    282\u001b[0m         \u001b[39mfor\u001b[39;00m raw_line \u001b[39min\u001b[39;00m chunk\u001b[39m.\u001b[39msplitlines():\n\u001b[1;32m    283\u001b[0m             line \u001b[39m=\u001b[39m raw_line\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/openai/_streaming.py:291\u001b[0m, in \u001b[0;36mSSEDecoder._iter_chunks\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given an iterator that yields raw binary data, iterate over it and yield individual SSE chunks\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 291\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m    292\u001b[0m     \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m chunk\u001b[39m.\u001b[39msplitlines(keepends\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m    293\u001b[0m         data \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m line\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpx/_models.py:829\u001b[0m, in \u001b[0;36mResponse.iter_bytes\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    827\u001b[0m chunker \u001b[39m=\u001b[39m ByteChunker(chunk_size\u001b[39m=\u001b[39mchunk_size)\n\u001b[1;32m    828\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request):\n\u001b[0;32m--> 829\u001b[0m     \u001b[39mfor\u001b[39;00m raw_bytes \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miter_raw():\n\u001b[1;32m    830\u001b[0m         decoded \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39mdecode(raw_bytes)\n\u001b[1;32m    831\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunker\u001b[39m.\u001b[39mdecode(decoded):\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpx/_models.py:883\u001b[0m, in \u001b[0;36mResponse.iter_raw\u001b[0;34m(self, chunk_size)\u001b[0m\n\u001b[1;32m    880\u001b[0m chunker \u001b[39m=\u001b[39m ByteChunker(chunk_size\u001b[39m=\u001b[39mchunk_size)\n\u001b[1;32m    882\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_request):\n\u001b[0;32m--> 883\u001b[0m     \u001b[39mfor\u001b[39;00m raw_stream_bytes \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream:\n\u001b[1;32m    884\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_bytes_downloaded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(raw_stream_bytes)\n\u001b[1;32m    885\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m chunker\u001b[39m.\u001b[39mdecode(raw_stream_bytes):\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpx/_client.py:126\u001b[0m, in \u001b[0;36mBoundSyncStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m typing\u001b[39m.\u001b[39mIterator[\u001b[39mbytes\u001b[39m]:\n\u001b[0;32m--> 126\u001b[0m     \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream:\n\u001b[1;32m    127\u001b[0m         \u001b[39myield\u001b[39;00m chunk\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpx/_transports/default.py:114\u001b[0m, in \u001b[0;36mResponseStream.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m    113\u001b[0m     \u001b[39mfor\u001b[39;00m part \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_httpcore_stream:\n\u001b[0;32m--> 114\u001b[0m         \u001b[39myield\u001b[39;00m part\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    135\u001b[0m     value \u001b[39m=\u001b[39m typ()\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen\u001b[39m.\u001b[39;49mthrow(typ, value, traceback)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    139\u001b[0m     \u001b[39m# Suppress StopIteration *unless* it's the same exception that\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[39m# was passed to throw().  This prevents a StopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[39m# raised inside the \"with\" statement from being suppressed.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m value\n",
      "File \u001b[0;32m~/Desktop/Mortal/mortal/lib/python3.9/site-packages/httpx/_transports/default.py:86\u001b[0m, in \u001b[0;36mmap_httpcore_exceptions\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[1;32m     85\u001b[0m message \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(exc)\n\u001b[0;32m---> 86\u001b[0m \u001b[39mraise\u001b[39;00m mapped_exc(message) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "\u001b[0;31mRemoteProtocolError\u001b[0m: peer closed connection without sending complete message body (incomplete chunked read)"
     ]
    }
   ],
   "source": [
    "summary = \"\"\n",
    "request = HumanMessage(\n",
    "    content=f\"Write a 3-page paper on the code provided, which is a segmentation model used to detect centerlines of dendrites under microscope\\n {code}\",\n",
    ")\n",
    "for chunk in generate.stream({\"messages\": [request]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    summary += chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflection_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a teacher grading a paper about a deep learning project. Generate critique and recommendations for the user's submission.\"\n",
    "            f\"The overall paper structure can be referred to {demo}.\"\n",
    "            \" Provide detailed recommendations, including requests for length, degree of details, clarity and format, etc.\"\n",
    "            \n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "reflect = reflection_prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Deep Learning Model for Centerline Detection of Neuronal Dendrites\n",
      "\n",
      "Authors: [Your Name(s)]\n",
      "\n",
      "Abstract:\n",
      "This paper presents a deep learning model for the detection of centerlines in neuronal dendrite images. The model employs a customized Gabor U-Net architecture, designed to capture intricate structural details in high-depth grayscale images. The model is trained and evaluated on a dataset consisting of 28,634 16-bit grayscale images, with corresponding segmentation masks. We explore various data augmentation techniques, model architectures, and loss functions to improve the model's performance. The best-performing model achieves a high F1 score of 0.9532, demonstrating its effectiveness in accurately identifying and segmenting the central lines of membrane structures.\n",
      "\n",
      "1. Introduction\n",
      "The automatic detection of centerlines in neuronal dendrite images is a crucial task in understanding the morphology and connectivity of neuronal networks. This paper introduces a deep learning model based on a customized Gabor U-Net architecture for accurate centerline detection in high-depth grayscale images.\n",
      "\n",
      "2. Data Preparation\n",
      "2.1 Data Collection\n",
      "The dataset consists of 28,634 16-bit grayscale images of size 65x65 pixels, with corresponding segmentation masks. Each image represents a membrane structure with intensity values ranging from approximately 150 to 1000, while the background intensity ranges from approximately 110 to 150.\n",
      "\n",
      "2.2 Data Preprocessing\n",
      "Data preprocessing includes adjusting intensity ranges, handling empty samples, normalizing, and applying data augmentation techniques such as elastic deformation, horizontal and vertical flips, and patch construction.\n",
      "\n",
      "2.3 Data Splitting\n",
      "The dataset is split into training, validation, and test sets with a ratio of 70% training, 15% validation, and 15% testing.\n",
      "\n",
      "3. Model Development\n",
      "3.1 Model Selection\n",
      "The U-Net architecture is chosen for its effectiveness in biomedical image segmentation tasks, particularly in capturing both spatial context and fine-grained details.\n",
      "\n",
      "3.2 Model Architecture\n",
      "The customized Gabor U-Net architecture consists of an encoding path, a bottleneck, and a decoding path. The encoding path captures increasingly complex features using max-pooling layers and convolutional layers with ReLU activations and batch normalization. The bottleneck captures the most abstract features, while the decoding path combines features from the corresponding encoding block through concatenation, followed by convolutional layers.\n",
      "\n",
      "3.3 Training\n",
      "3.3.1 Training Setup\n",
      "The model is trained using the Adam optimizer, binary cross-entropy loss, and Dice loss. The training loop includes a forward pass, loss calculation, backward pass, weight update, and monitoring of training and validation loss and metrics.\n",
      "\n",
      "3.4 Testing\n",
      "3.4.1 Model Evaluation\n",
      "The model is evaluated based on Intersection over Union (IoU), Dice coefficient, precision, recall, and F1 score.\n",
      "\n",
      "3.4.2 Visualization\n",
      "Segmentation results and metric visualization are presented to qualitatively and quantitatively assess the model's performance.\n",
      "\n",
      "4. Inference\n",
      "4.1 Loading the Trained Model\n",
      "The trained model's weights are loaded for inference on new data.\n",
      "\n",
      "4.2 Running Inference on New Data\n",
      "New 16-bit grayscale images are prepared, normalized, and passed through the loaded model to obtain predicted segmentation masks. Post-processing steps, such as thresholding and morphological operations, are applied to refine the predicted masks.\n",
      "\n",
      "4.3 Visualization of Inferred Segmentation Masks\n",
      "Inferred segmentation masks are visualized alongside the original images to qualitatively assess the model's performance on unseen samples.\n",
      "\n",
      "5. Results\n",
      "The models are evaluated based on several metrics, with a particular focus on the balance between precision and recall. The Normal U-Net with BCE Loss achieves the highest F1 score of 0.9532, indicating the best overall performance in balancing precision and recall.\n",
      "\n",
      "6. Conclusion\n",
      "The customized Gabor U-Net model demonstrates strong performance in detecting centerlines in neuronal dendrite images, providing a valuable tool for the analysis of neuronal morphology and connectivity. Future work includes further optimization, exploration of additional model architectures, and application to larger and more diverse datasets.\n",
      "\n",
      "Critique and Recommendations:\n",
      "\n",
      "1. Data Preparation:\n",
      "- It is good to see that the data preprocessing steps are well-documented. However, it would be helpful to provide more details on the specific intensity range adjustments made and the rationale behind them.\n",
      "- Consider providing a justification for the choice of data augmentation techniques and their parameters.\n",
      "\n",
      "2. Model Development:\n",
      "- The paper could benefit from a more detailed explanation of the customized Gabor U-Net architecture, including the motivation for the chosen design and the impact of the Gabor filters on the model's performance.\n",
      "- It would be helpful to include a table or figure summarizing the different U-Net variations and their configurations.\n",
      "\n",
      "3. Training:\n",
      "- The training setup is generally well-explained. However, it would be beneficial to provide more information on the choice of learning rate, batch size, and number of epochs.\n",
      "- Consider adding a learning rate scheduler to further improve the model's convergence and performance.\n",
      "\n",
      "4. Testing:\n",
      "- The evaluation metrics are well-chosen. However, it would be helpful to include more details on how the metrics are calculated and their significance in the context of this project.\n",
      "- Consider adding a confusion matrix or a ROC curve to provide a more comprehensive evaluation of the model's performance.\n",
      "\n",
      "5. Inference:\n",
      "- The inference process is well-described. However, it would be helpful to provide more details on the post-processing steps and their impact on the final results.\n",
      "\n",
      "6. Results:\n",
      "- The results are presented clearly, but it would be beneficial to include more visual examples of the model's performance on both successful and unsuccessful cases.\n",
      "- Consider comparing the customized Gabor U-Net model to other state-of-the-art segmentation models to provide a better understanding of its performance and limitations.\n",
      "\n",
      "7. Conclusion:\n",
      "- The conclusion is well-written, but it would be helpful to include a discussion on potential limitations and future improvements for the model.\n",
      "- Consider mentioning potential applications or extensions of this work, such as real-time centerline detection or integration with other neuronal analysis tools."
     ]
    }
   ],
   "source": [
    "reflection = \"\"\n",
    "for chunk in reflect.stream({\"messages\": [request, HumanMessage(content=summary)]}):\n",
    "    print(chunk.content, end=\"\")\n",
    "    reflection += chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Deep Learning Model for Centerline Detection of Neuronal Dendrites\n",
      "\n",
      "Authors: [Your Name(s)]\n",
      "\n",
      "Abstract:\n",
      "This paper presents a deep learning model for the detection of centerlines in neuronal dendrite images. The model employs a customized Gabor U-Net architecture, designed to capture intricate structural details in high-depth grayscale images. The model is trained and evaluated on a dataset consisting of 28,634 16-bit grayscale images, with corresponding segmentation masks. We explore various data augmentation techniques, model architectures, and loss functions to improve the model's performance. The best-performing model achieves a high F1 score of 0.9532, demonstrating its effectiveness in accurately identifying and segmenting the central lines of membrane structures.\n",
      "\n",
      "1. Introduction\n",
      "The automatic detection of centerlines in neuronal dendrite images is a crucial task in understanding the morphology and connectivity of neuronal networks. This paper introduces a deep learning model based on a customized Gabor U-Net architecture for accurate centerline detection in high-depth grayscale images.\n",
      "\n",
      "2. Data Preparation\n",
      "2.1 Data Collection\n",
      "The dataset consists of 28,634 16-bit grayscale images of size 65x65 pixels, with corresponding segmentation masks. Each image represents a membrane structure with intensity values ranging from approximately 150 to 1000, while the background intensity ranges from approximately 110 to 150.\n",
      "\n",
      "2.2 Data Preprocessing\n",
      "Data preprocessing includes adjusting intensity ranges by setting the background intensity range to [100, 150] and the membrane intensity range to [150, 1010]. Handling empty samples involves reducing the number of empty samples from 1,000 to 100 (3.5% to 0.35% of the total dataset). Normalization is performed using MINMAX Normalization to scale pixel values to the [0, 1] range. Data augmentation techniques include elastic deformation, horizontal and vertical flips, and patch construction. However, the results of patch construction were suboptimal due to information loss and increased input size.\n",
      "\n",
      "2.3 Data Splitting\n",
      "The dataset is split into training, validation, and test sets with a ratio of 70% training, 15% validation, and 15% testing.\n",
      "\n",
      "3. Model Development\n",
      "3.1 Model Selection\n",
      "The U-Net architecture is chosen for its effectiveness in biomedical image segmentation tasks, particularly in capturing both spatial context and fine-grained details.\n",
      "\n",
      "3.2 Model Architecture\n",
      "The customized Gabor U-Net architecture consists of an encoding path, a bottleneck, and a decoding path. The encoding path captures increasingly complex features using max-pooling layers and convolutional layers with ReLU activations and batch normalization. The bottleneck captures the most abstract features, while the decoding path combines features from the corresponding encoding block through concatenation, followed by convolutional layers. Gabor filters are applied in the encoding path to enhance the model's ability to capture local and global features.\n",
      "\n",
      "Table 1: Summary of U-Net Variations\n",
      "\n",
      "| Model          | Layers  | Input Size | Output Size |\n",
      "|----------------|---------|------------|--------------|\n",
      "| Normal U-Net   | 1,931,201| 65x65x1    | 65x65x1      |\n",
      "| Mini U-Net     | 471,361  | 65x65x1    | 65x65x1      |\n",
      "| Extended U-Net | 1,933,577| 65x65x1    | 65x65x1      |\n",
      "| U-Net SA       | 1,958,364| 65x65x1    | 65x65x1      |\n",
      "| Mini U-Net SA  | 477,883  | 65x65x1    | 65x65x1      |\n",
      "\n",
      "3.3 Training\n",
      "3.3.1 Training Setup\n",
      "The model is trained using the Adam optimizer with an initial learning rate of 1e-3, decreasing to 1e-4 based on convergence behavior. The batch size is set to 128 (64 on local GPU), and the number of epochs is set to 50.\n",
      "\n",
      "3.4 Testing\n",
      "3.4.1 Model Evaluation\n",
      "The model is evaluated based on Intersection over Union (IoU), Dice coefficient, precision, recall, and F1 score.\n",
      "\n",
      "3.4.2 Visualization\n",
      "Segmentation results and metric visualization are presented to qualitatively and quantitatively assess the model's performance.\n",
      "\n",
      "4. Inference\n",
      "4.1 Loading the Trained Model\n",
      "The trained model's weights are loaded for inference on new data.\n",
      "\n",
      "4.2 Running Inference on New Data\n",
      "New 16-bit grayscale images are prepared, normalized, and passed through the loaded model to obtain predicted segmentation masks. Post-processing steps, such as thresholding and morphological operations, are applied to refine the predicted masks.\n",
      "\n",
      "4.3 Visualization of Inferred Segmentation Masks\n",
      "Inferred segmentation masks are visualized alongside the original images to qualitatively assess the model's performance on unseen samples.\n",
      "\n",
      "5. Results\n",
      "The models are evaluated based on several metrics, with a particular focus on the balance between precision and recall. The Normal U-Net with BCE Loss achieves the highest F1 score of 0.9532, indicating the best overall performance in balancing precision and recall.\n",
      "\n",
      "6. Conclusion\n",
      "The customized Gabor U-Net model demonstrates strong performance in detecting centerlines in neuronal dendrite images, providing a valuable tool for the analysis of neuronal morphology and connectivity. Future work includes further optimization, exploration of additional model architectures, and application to larger and more diverse datasets.\n",
      "\n",
      "Critique and Recommendations:\n",
      "\n",
      "1. Data Preparation:\n",
      "- Provide more details on the specific intensity range adjustments made and the rationale behind them.\n",
      "- Justify the choice of data augmentation techniques and their parameters.\n",
      "\n",
      "2. Model Development:\n",
      "- Explain the customized Gabor U-Net architecture in more detail, including the motivation for the chosen design and the impact of the Gabor filters on the model's performance.\n",
      "- Include a table or figure summarizing the different U-Net variations and their configurations.\n",
      "\n",
      "3. Training:\n",
      "- Provide more information on the choice of learning rate, batch size, and number of epochs.\n",
      "- Add a learning rate scheduler to further improve the model's convergence and performance.\n",
      "\n",
      "4. Testing:\n",
      "- Include more details on how the metrics are calculated and their significance in the context of this project.\n",
      "- Add a confusion matrix or a ROC curve to provide a more comprehensive evaluation of the model's performance.\n",
      "\n",
      "5. Inference:\n",
      "- Provide more details on the post-processing steps and their impact on the final results.\n",
      "\n",
      "6. Results:\n",
      "- Include more visual examples of the model's performance on both successful and unsuccessful cases.\n",
      "- Compare the customized Gabor U-Net model to other state-of-the-art segmentation models to provide a better understanding of its performance and limitations.\n",
      "\n",
      "7. Conclusion:\n",
      "- Discuss potential limitations and future improvements for the model.\n",
      "- Mention potential applications or extensions of this work, such as real-time centerline detection or integration with other neuronal analysis tools."
     ]
    }
   ],
   "source": [
    "updated_paper = \"\"\n",
    "for chunk in generate.stream(\n",
    "    {\"messages\": [request, AIMessage(content=summary), HumanMessage(content=reflection)]}\n",
    "):\n",
    "    print(chunk.content, end=\"\")\n",
    "    updated_paper += chunk.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an summary on the code provided, which is a segmentation model used to detect centerlines of dendrites under microscope\n",
      " \n",
      "\n",
      "[Python code for training the model]\n",
      "\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from torch.optim.lr_scheduler import StepLR\n",
      "from data import CenterlineDataset\n",
      "from gabor_unet_nfc import GaborUNet\n",
      "import argparse\n",
      "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
      "from torchvision import transforms\n",
      "import matplotlib.pyplot as plt\n",
      "from torch import nn\n",
      "import os\n",
      "import datetime\n",
      "import uuid\n",
      "import math\n",
      "from scipy.ndimage import distance_transform_edt\n",
      "import numpy as np\n",
      "from unet_model import UNet\n",
      "import torch\n",
      "from torchvision.transforms import functional as TF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "# class WeightedBCELoss(nn.Module):\n",
      "#     '''Less weight is given to pixels in the dilated area but not in the original target.'''\n",
      "#     def __init__(self, custom_weight=0.5, loss_function='bce'):\n",
      "#         super(WeightedBCELoss, self).__init__()\n",
      "#         self.custom_weight = custom_weight\n",
      "#         self.loss_function = loss_function\n",
      "\n",
      "#     def forward(self, prediction, label, dilated_label):\n",
      "#         # Calculate weights: pixels in dilated area but not in original target are weighted less\n",
      "#         # Higher weight \n",
      "#         weight = torch.ones_like(label) # shape: [batch_size, 1, 64, 64]\n",
      "#         # weight[(dilated_label == 1) & (label == 0)] = self.custom_weight\n",
      "#         weight[(label==1)] = self.custom_weight\n",
      "        \n",
      "#         if self.loss_function == 'bce':\n",
      "#             loss = F.binary_cross_entropy(prediction, label, weight=weight)\n",
      "#         elif self.loss_function == 'nll':\n",
      "#             loss = F.nll_loss(prediction, label, weight=weight)\n",
      "#         return loss\n",
      "\n",
      "\n",
      "class To16BitTensor:\n",
      "    def __call__(self, image):\n",
      "        image_tensor = TF.to_tensor(image)\n",
      "        image_tensor = image_tensor / 65535.0\n",
      "        return image_tensor\n",
      "\n",
      "class Normalize16BitRange(transforms.ToTensor):\n",
      "    def __call__(self, pic):\n",
      "        '''\n",
      "            Input: PIL Image or numpy array\n",
      "            Output: Tensor Normalized to [0, 1] based on the actual data range\n",
      "        '''\n",
      "        img = torch.from_numpy(np.array(pic, dtype=np.float32, copy=True))\n",
      "        \n",
      "        # mimic ToTensor having shape [C, H, W]\n",
      "        if img.dim() == 2:\n",
      "            img = img.unsqueeze(0)\n",
      "\n",
      "        # Normalization based on the actual data range\n",
      "        # min_val = torch.min(img)\n",
      "        # max_val = torch.max(img)\n",
      "        # min_val = 108\n",
      "        max_val = 2000\n",
      "        # pixels greater than 2826 fixed at 2826\n",
      "        img = torch.where(img > max_val, torch.tensor(max_val), img)\n",
      "        # pixels less than 90 fixed at 90\n",
      "        # img = torch.where(img < min_val, torch.tensor(min_val), img)\n",
      "        img = img / max_val  # Normalize to [0, 1]\n",
      "\n",
      "        return img\n",
      "\n",
      "# class Normalize16Bit(transforms.ToTensor):\n",
      "#     def __call__(self, pic):\n",
      "#         img_np = np.array(pic, dtype=np.float32, copy=True)\n",
      "#         min_np = np.min(img_np)\n",
      "#         max_np = np.max(img_np)\n",
      "\n",
      "#         img_np = (img_np - min_np) / (max_np - min_np)  # Normalize to [0, 1]\n",
      "\n",
      "#         return img_np\n",
      "       \n",
      "class WeightedBCELoss(nn.Module):\n",
      "    def __init__(self, weight=None):\n",
      "        super().__init__()\n",
      "        self.custom_weight = weight\n",
      "    def forward(self, prediction, label):\n",
      "        \n",
      "        # Apply the general weighted binary cross entropy loss function, assigning higher penality to false negatives.\n",
      "        # The purpose here is to handle imbalanced datasets, where the number of negative pixels is much higher than the number of positive pixels. \n",
      "        \n",
      "        weight = torch.ones_like(label)\n",
      "        # Handle class imbalance\n",
      "        # GET THE WEIGHT\n",
      "        if self.custom_weight is not None:\n",
      "            weight[(label == 1)] = self.custom_weight\n",
      "        else:        \n",
      "            # calculate the number of all pixles in the label \n",
      "            total_pixels = label.numel()\n",
      "            # calculate the number of positive pixels in the label\n",
      "            positive_pixels = label.sum().item()\n",
      "            # calculate the number of negative pixels in the label\n",
      "            negative_pixels = total_pixels - positive_pixels\n",
      "            # calculate the weights for the positive pixels and negative pixels\n",
      "            weight_positive_prime = max(math.log(2 * total_pixels / (positive_pixels + 1e-6)), 1.0)\n",
      "            weight_negative_prime = max(math.log(2 * total_pixels / (negative_pixels+ 1e-6)), 1.0)\n",
      "            weight_positive = weight_positive_prime / (weight_positive_prime + weight_negative_prime)\n",
      "            weight_negative = weight_negative_prime / (weight_positive_prime + weight_negative_prime)\n",
      "            # assign the weights to the pixels\n",
      "            weight[label == 1] = weight_positive\n",
      "            weight[label == 0] = weight_negative\n",
      "        \n",
      "            # Assign significance per pixel based on their location\n",
      "            # find the distance from each pixel to the nearest centerline pixel\n",
      "            # label_np = label.numpy()\n",
      "            # binary_mask = (label_np == 0).astype(np.int) # 1 for background, 0 for centerline\n",
      "            # distance_map_np = distance_transform_edt(binary_mask)\n",
      "            # distance_map = torch.from_numpy(distance_map_np)\n",
      "            # GPU compatible version\n",
      "            label_np = label.cpu().numpy()\n",
      "            binary_mask = (label_np == 0).astype(int) # 1 for background, 0 for centerline\n",
      "            distance_map_np = distance_transform_edt(binary_mask)\n",
      "            distance_map = torch.from_numpy(distance_map_np).to(label.device)\n",
      "            weight = weight * (1 - 0.01) ** distance_map\n",
      "        \n",
      "        # print(\"The data type of prediction is: \", prediction.dtype)\n",
      "        # print(\"The data type of label is: \", label.dtype)\n",
      "\n",
      "        loss = F.binary_cross_entropy(prediction, label, weight=weight) \n",
      "        return loss\n",
      "\n",
      "\n",
      "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
      "    # state here is a dictionary containing the model's state_dict, the optimizer's state_dict, and the epoch number\n",
      "    torch.save(state, filename)\n",
      "    print(f\"Checkpoint saved to {filename}\")\n",
      "\n",
      "\n",
      "def load_checkpoint(checkpoint_dir, model, optimizer, device):\n",
      "    latest_checkpoint = None\n",
      "    max_epoch = -1\n",
      "    for file in os.listdir(checkpoint_dir):\n",
      "        if file.startswith('checkpoint') and file.endswith('.pth.tar'):\n",
      "            epoch_num = int(file.split('_')[-1].split('.')[0])\n",
      "            if epoch_num > max_epoch:\n",
      "                max_epoch = epoch_num\n",
      "                latest_checkpoint = file\n",
      "    \n",
      "    if latest_checkpoint is not None:\n",
      "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
      "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
      "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "        model.load_state_dict(checkpoint['state_dict'])\n",
      "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
      "        start_epoch = checkpoint['epoch']\n",
      "        print(f\"Checkpoint loaded. Starting from epoch {start_epoch}\")\n",
      "    else:\n",
      "        print(\"No checkpoint found. Starting from scratch.\")\n",
      "        start_epoch = 1\n",
      "    \n",
      "    return model, optimizer, start_epoch\n",
      "\n",
      "\n",
      "def train(args, model, device, train_loader, optimizer, epoch, custom_loss=None, log_name='Analysis/logs/log.txt', checkpoint_dir = 'checkpoints'):\n",
      "    log_messages = []\n",
      "    model.train()\n",
      "    train_loss = 0\n",
      "    for batch_idx, data_tuple in enumerate(train_loader):\n",
      "        # Unpack the data_tuple and check for None values\n",
      "        if any(x is None for x in data_tuple):\n",
      "            print(f\"Skipping batch {batch_idx} due to None values\")\n",
      "            log_messages.append(f\"Skipping batch {batch_idx} due to None values\")\n",
      "            continue  # Skip this batch\n",
      "        \n",
      "        data, target = (x.to(device) for x in data_tuple)\n",
      "        optimizer.zero_grad()\n",
      "        output = model(data)\n",
      "        # print(target.shape)\n",
      "        # print(output.shape)\n",
      "        # print(target.dtype)\n",
      "        # print(output.dtype)\n",
      "        # print(\"The max value of the target is: \", torch.max(target).item())\n",
      "        # print(\"The min value of the target is: \", torch.min(target).item())\n",
      "        # print(\"The max value of the output is: \", torch.max(output).item())\n",
      "        # print(\"The min value of the output is: \", torch.min(output).item())\n",
      "        if not custom_loss:\n",
      "            loss = F.binary_cross_entropy(output, target)\n",
      "        else:\n",
      "            # loss = custom_loss(output, target, dilated_target)\n",
      "            loss = custom_loss(output, target)\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "        train_loss += loss.item()\n",
      "        if batch_idx % args.log_interval == 0:\n",
      "            message = f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\tLoss (Batch): {loss.item():.6f}'\n",
      "            print(message)\n",
      "            # add to log \n",
      "            log_messages.append(message)\n",
      "\n",
      "    with open(log_name, 'a') as log_file:\n",
      "        for message in log_messages:\n",
      "            log_file.write(f\"{'*'*20} Training Epoch: {epoch} {'*'*20}\n",
      "\")\n",
      "            log_file.write(message + '\n",
      "')\n",
      "\n",
      "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth.tar\")\n",
      "    save_checkpoint({\n",
      "        'epoch': epoch,\n",
      "        'state_dict': model.state_dict(),\n",
      "        'optimizer': optimizer.state_dict(),\n",
      "    }, filename=checkpoint_path)\n",
      "\n",
      "    average_loss = train_loss / len(train_loader)\n",
      "    return average_loss\n",
      "\n",
      "\n",
      "def test(args, model, device, test_loader, epoch, custom_loss=None, log_name='Analysis/logs/log.txt', loss_function='bce'):\n",
      "    log_messages = []\n",
      "    model.eval()\n",
      "    test_loss = 0\n",
      "    total_correct_pixels = 0\n",
      "    total_centerline_pixels = 0\n",
      "    test_f1_score = 0\n",
      "    with torch.no_grad():\n",
      "        for batch_idx, data_tuple in enumerate(test_loader):\n",
      "            if any(x is None for x in data_tuple):\n",
      "                print(f\"Skipping batch {batch_idx} due to None values\")\n",
      "                log_messages.append(f\"Skipping batch {batch_idx} due to None values\")\n",
      "                continue \n",
      "            \n",
      "            data, target = (x.to(device) for x in data_tuple)\n",
      "            output = model(data)\n",
      "            if not custom_loss:\n",
      "                loss = F.binary_cross_entropy(output, target, reduction='sum').item()\n",
      "            else:\n",
      "                # loss = custom_loss(output, target, dilated_target).item()\n",
      "                loss = custom_loss(output, target).item()\n",
      "            test_loss += loss\n",
      "            # pred = output > 0.5\n",
      "            # correct_pixels = pred.eq(target.view_as(pred)).sum().item()\n",
      "            # total_correct_pixels += correct_pixels\n",
      "            # total_pixels += target.numel()\n",
      "            # accuracy = 100. * correct_pixels / target.numel()\n",
      "\n",
      "            # accuracy = the number of corrected predicted centerline pixels divided by the number of centerline pixels in the target\n",
      "            pred_binary = (output > 0.5).type(torch.bool)\n",
      "            target_binary = target.type(torch.bool)\n",
      "\n",
      "            # centerline_pixels = target_binary.sum().item()\n",
      "            # total_centerline_pixels += centerline_pixels\n",
      "            # correct_pixels = (pred_binary & target_binary).sum().item()\n",
      "            # total_correct_pixels += correct_pixels\n",
      "\n",
      "            # if centerline_pixels > 0:\n",
      "            #     accuracy = 100. * correct_pixels / centerline_pixels\n",
      "            # else:\n",
      "            #     accuracy = 100\n",
      "\n",
      "            TP = (pred_binary & target_binary).sum().item()\n",
      "            FP = (pred_binary & ~target_binary).sum().item()\n",
      "            FN = (~pred_binary & target_binary).sum().item()\n",
      "\n",
      "            # calculate accuracy, precision, and recall\n",
      "            if (TP + FN) > 0:\n",
      "                recall = 100. * TP / (TP + FN)\n",
      "            else:\n",
      "                recall = 0\n",
      "            \n",
      "            if (TP + FP) > 0:\n",
      "                precision = 100. * TP / (TP + FP)\n",
      "            else:\n",
      "                precision = 0\n",
      "            \n",
      "            # Calculate F1 Score\n",
      "            if (precision + recall) > 0:\n",
      "                f1_score = 2 * (precision * recall) / (precision + recall)\n",
      "            else:\n",
      "                f1_score = 0\n",
      "                \n",
      "            test_f1_score += f1_score\n",
      "            total_centerline_pixels += target_binary.sum().item()\n",
      "            total_correct_pixels += TP\n",
      "\n",
      "            if batch_idx % args.log_interval == 0:\n",
      "                message = f'Test Epoch: {epoch} [{batch_idx * len(data)}/{len(test_loader.dataset)} ({100. * batch_idx / len(test_loader):.0f}%)]\tLoss (Batch): {loss:.6f} \t Accuracy (Batch): {recall:.2f}% \t F1 Score (Batch): {f1_score:.2f}%'\n",
      "                print(message)\n",
      "                # add to log\n",
      "                log_messages.append(message)\n",
      "\n",
      "    with open(log_name, 'a') as log_file:\n",
      "        for message in log_messages:\n",
      "            log_file.write(f\"\n",
      "{'*'*20} Testing Epoch: {epoch} {'*'*20}\n",
      "\")\n",
      "            log_file.write(message + '\n",
      "')     \n",
      "\n",
      "    average_loss = test_loss / len(test_loader)  # Calculate average loss per batch\n",
      "    average_accuracy = 100. * total_correct_pixels / total_centerline_pixels  # Calculate recall (accuracy)\n",
      "    average_f1_score = test_f1_score / len(test_loader)  # Calculate average F1 score per batch\n",
      "    return average_loss, average_accuracy, average_f1_score\n",
      "\n",
      "\n",
      "def main():\n",
      "    \n",
      "    # COMMAND LINE ARGUMENTS\n",
      "    parser = argparse.ArgumentParser(description='PyTorch GaborUNet Training')\n",
      "    parser.add_argument('--batch-size', type=int, default=16, metavar='N',\n",
      "                        help='input batch size for training (default: 16)')\n",
      "    parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
      "                        help='input batch size for testing (default: 100)')\n",
      "    parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
      "                        help='number of epochs to train (default: 50)')\n",
      "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
      "                        help='learning rate (default: 0.01)')\n",
      "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
      "                        help='learning rate step gamma (default: 0.7)')\n",
      "    parser.add_argument('--step-size', type=int, default=1, metavar='N',)\n",
      "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
      "                        help='disables CUDA training')\n",
      "    # parser.add_argument('--save-dilated', action='store_true', default=False, \n",
      "    #                     help='save dilated labels for debugging')\n",
      "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
      "                        help='how many batches to wait before logging training status')\n",
      "    parser.add_argument('--custom-weight', action='store_true', default=False,\n",
      "                        help='less penality for pixels in dilated area but not in original target')\n",
      "    parser.add_argument('--weight', type=float, default=8, help='Custom weight for the BC or NLL loss for pixels in dilated area but not in the original target')\n",
      "    parser.add_argument('--weight-decay', type=float, default=1e-5, help='Weight decay for optimizer')\n",
      "    # parser.add_argument('--add-poor-quality-training', action='store_true', default=False, help='Add poor quality training data to the dataset')\n",
      "    parser.add_argument('--turn-off-analysis', action='store_true', default=False, help='Turn off analysis of the dataset (saving the loss image)')\n",
      "    parser.add_argument('--gabor-kernel-size', type=int, default=19, help='Size of the Gabor kernel')\n",
      "    parser.add_argument('--message', type=str, default='', help='Additional message to add to the log')\n",
      "    # parser.add_argument('--dilation-iterations', type=int, default=1, help='Number of dilation iterations for the dilated label')\n",
      "    # parser.add_argument('--loss-function' , type=str, default='bce', help='Loss function to use (bce or nll)')\n",
      "    parser.add_argument('--train-from-checkpoint', type=str, default=None, help='Input the logid of the the checkpoint to resume training')\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    log_messages = []\n",
      "    # generate a unique id \n",
      "    log_id = uuid.uuid4() if args.train_from_checkpoint is None else args.train_from_checkpoint\n",
      "    log_name = f'Analysis/logs/log-{datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")}-{log_id}.txt'\n",
      "\n",
      "    # add to log\n",
      "    log_messages.append(f'Log ID: {log_id}; Message: {args.message}')\n",
      "    log_messages.append(f'Starting time: {datetime.datetime.now()}')\n",
      "    log_messages.append(f\"{'*'*20}\")\n",
      "    log_messages.append(f\"Batch size: {args.batch_size}, help='input batch size for training (default: 16)\")\n",
      "    log_messages.append(f\"Number of epochs: {args.epochs}, help='number of epochs to train (default: 50)\")\n",
      "    log_messages.append(f\"Learning rate: {args.lr}, help='learning rate (default: 0.01)\")\n",
      "    log_messages.append(f\"Learning rate step gamma: {args.gamma}, help='learning rate step gamma (default: 0.7)\")\n",
      "    log_messages.append(f\"Learning rate step size: {args.step_size}, help='learning rate step size\")\n",
      "    # log_messages.append(f\"Save dilated labels: {args.save_dilated}, help='save dilated labels for debugging\")\n",
      "    log_messages.append(f\"Log interval: {args.log_interval}, help='how many batches to wait before logging training status\")\n",
      "    log_messages.append(f\"Custom weight: {args.custom_weight}, help='less penality for pixels in dilated area but not in original target\")\n",
      "    log_messages.append(f\"weight: {args.weight}, help='Custom weight for the BCE OR NLL loss for pixels in dilated area but not in the original target\")\n",
      "    log_messages.append(f\"Weight decay: {args.weight_decay}, help='Weight decay for optimizer\")\n",
      "    # log_messages.append(f\"Add poor quality training: {args.add_poor_quality_training}, help='Add poor quality training data to the dataset\")\n",
      "    log_messages.append(f\"Turn off analysis: {args.turn_off_analysis}, help='Turn off analysis of the dataset (saving the loss image)\")\n",
      "    log_messages.append(f\"Gabor kernel size: {args.gabor_kernel_size}, help='Size of the Gabor kernel\")\n",
      "    # log_messages.append(f\"Dilation iterations: {args.dilation_iterations}, help='Number of dilation iterations for the dilated label\")\n",
      "    # log_messages.append(f\"Loss function: {args.loss_function}, help='Loss function to use (bce or nll)\")\n",
      "    log_messages.append(f\"Train from checkpoint: {args.train_from_checkpoint}, help='Input the logid of the the checkpoint to resume training\")\n",
      "    log_messages.append(f\"{'*'*20}\")\n",
      "\n",
      "\n",
      "    if args.custom_weight:\n",
      "        print(f\"Using custom loss function with weight: {args.weight}\")\n",
      "        log_messages.append(f\"Using custom loss function with weight: {args.weight}\")\n",
      "\n",
      "    # LOAD DATASET\n",
      "    # data_dir='data_lq_thick_bg_3'\n",
      "    # # data_poor_quality_dir='data_poor_quality'\n",
      "    # img_dir=f\"{data_dir}/samples\"\n",
      "    # label_dir=f\"{data_dir}/labels\"\n",
      "    # img_dir_poor_quality=f\"{data_poor_quality_dir}/samples\"\n",
      "    # label_dir_poor_quality=f\"{data_poor_quality_dir}/labels\"\n",
      "    # dilated_label_dir=f\"{data_dir}/dilated_labels\"\n",
      "\n",
      "    # if args.add_poor_quality_training:\n",
      "    #     # check if there are folders \n",
      "    #     if not os.path.exists(data_poor_quality_dir) or not os.path.exists(img_dir_poor_quality) or not os.path.exists(label_dir_poor_quality):\n",
      "    #         print(\"Poor quality data not found. Please make sure the folder data_poor_quality exists and contains samples and labels folders.\")\n",
      "    #         print(\"Training stopped. Exiting program...\")\n",
      "    #         # stop the program\n",
      "    #         return\n",
      "        \n",
      "    # if not os.path.exists(data_dir) or not os.path.exists(img_dir) or not os.path.exists(label_dir):\n",
      "    #     print(\"Data not found. Please make sure the folder data exists and contains samples and labels folders.\")\n",
      "    #     print(\"Training stopped. Exiting program...\")\n",
      "    #     # stop the program\n",
      "    #     return\n",
      "\n",
      "    transform = transforms.Compose([\n",
      "        # transforms.RandomHorizontalFlip(),  \n",
      "        # transforms.RandomRotation(15),  \n",
      "        # transforms.ToTensor(),  \n",
      "        Normalize16BitRange(),\n",
      "        # Normalize16Bit(),\n",
      "        # transforms.ToTensor()\n",
      "    ])\n",
      "\n",
      "    # # dataset = CenterlineDataset(img_dir, label_dir, dilated_label_dir, transform=transform)\n",
      "    # original_dataset = CenterlineDataset(img_dir, label_dir, transform, augmentation=False)\n",
      "    # # rotated_dataset = CenterlineDataset(img_dir, label_dir, transform, augmentation=\"rotate\")\n",
      "    # flipped_dataset = CenterlineDataset(img_dir, label_dir, transform, augmentation=\"flip\")\n",
      "\n",
      "    # # Add masked data\n",
      "    # data_dir_masked = 'data_normal_masked'\n",
      "    # img_dir_masked = f\"{data_dir_masked}/samples\"\n",
      "    # label_dir_masked = f\"{data_dir_masked}/labels\"\n",
      "    # masked_dataset = CenterlineDataset(img_dir_masked, label_dir_masked, transform, augmentation=False)\n",
      "    # masked_dataset_flipped = CenterlineDataset(img_dir_masked, label_dir_masked, transform, augmentation=\"flip\")\n",
      "\n",
      "\n",
      "    # # add new bg data \n",
      "    # data_dir_bg = 'new_bg_dataset'\n",
      "    # img_dir_bg = f\"{data_dir_bg}/samples\"\n",
      "    # label_dir_bg = f\"{data_dir_bg}/labels\"\n",
      "    # bg_dataset = CenterlineDataset(img_dir_bg, label_dir_bg, transform, augmentation=False)\n",
      "    \n",
      "    # # add new branch point experimental data \n",
      "    # data_dir_branch = 'branch_point_dataset'\n",
      "    # img_dir_branch = f\"{data_dir_branch}/samples\"\n",
      "    # label_dir_branch = f\"{data_dir_branch}/labels\"\n",
      "    # branch_dataset = CenterlineDataset(img_dir_branch, label_dir_branch, transform, augmentation=False)\n",
      "    # branch_dataset_flipped = CenterlineDataset(img_dir_branch, label_dir_branch, transform, augmentation=\"flip\")\n",
      "\n",
      "\n",
      "    # hand-labelled simple_structure_dataset_dilated\n",
      "    data_dir_simple_structure_dilated = 'simple_structure_dataset_dilated'\n",
      "    img_dir_simple_structure_dilated = f\"{data_dir_simple_structure_dilated}/samples\"\n",
      "    label_dir_simple_structure_dilated = f\"{data_dir_simple_structure_dilated}/labels\"\n",
      "    simple_structure_dilated_dataset = CenterlineDataset(img_dir_simple_structure_dilated, label_dir_simple_structure_dilated, transform, augmentation=False)\n",
      "    flipped_simple_structure_dilated_dataset = CenterlineDataset(img_dir_simple_structure_dilated, label_dir_simple_structure_dilated, transform, augmentation=\"flip\")\n",
      "\n",
      "\n",
      "    # hand-labelled simple_structure_dataset_dilated_masked\n",
      "    data_dir_simple_structure_dilated_masked = 'simple_structure_dataset_dilated_masked'\n",
      "    img_dir_simple_structure_dilated_masked = f\"{data_dir_simple_structure_dilated_masked}/samples\"\n",
      "    label_dir_simple_structure_dilated_masked = f\"{data_dir_simple_structure_dilated_masked}/labels\"\n",
      "    simple_structure_dilated_masked_dataset = CenterlineDataset(img_dir_simple_structure_dilated_masked, label_dir_simple_structure_dilated_masked, transform, augmentation=False)\n",
      "    flipped_simple_structure_dilated_masked_dataset = CenterlineDataset(img_dir_simple_structure_dilated_masked, label_dir_simple_structure_dilated_masked, transform, augmentation=\"flip\")\n",
      "\n",
      "    # hand-labelled branch_point_dataset_dilated\n",
      "    data_dir_branch_point_dilated = 'branch_point_dataset_dilated'\n",
      "    img_dir_branch_point_dilated = f\"{data_dir_branch_point_dilated}/samples\"\n",
      "    label_dir_branch_point_dilated = f\"{data_dir_branch_point_dilated}/labels\"\n",
      "    branch_point_dilated_dataset = CenterlineDataset(img_dir_branch_point_dilated, label_dir_branch_point_dilated, transform, augmentation=False)\n",
      "    flipped_branch_point_dilated_dataset = CenterlineDataset(img_dir_branch_point_dilated, label_dir_branch_point_dilated, transform, augmentation=\"flip\")\n",
      "\n",
      "    # hand-labelled branch_point_dataset_dilated_masked\n",
      "    data_dir_branch_point_dilated_masked = 'branch_point_dataset_dilated_masked'\n",
      "    img_dir_branch_point_dilated_masked = f\"{data_dir_branch_point_dilated_masked}/samples\"\n",
      "    label_dir_branch_point_dilated_masked = f\"{data_dir_branch_point_dilated_masked}/labels\"\n",
      "    branch_point_dilated_masked_dataset = CenterlineDataset(img_dir_branch_point_dilated_masked, label_dir_branch_point_dilated_masked, transform, augmentation=False)\n",
      "    flipped_branch_point_dilated_masked_dataset = CenterlineDataset(img_dir_branch_point_dilated_masked, label_dir_branch_point_dilated_masked, transform, augmentation=\"flip\")\n",
      "\n",
      "    # hand labelled noisy_patch_dataset\n",
      "    data_dir_noisy_patch = 'noisy_patch_dataset'\n",
      "    img_dir_noisy_patch = f\"{data_dir_noisy_patch}/samples\"\n",
      "    label_dir_noisy_patch = f\"{data_dir_noisy_patch}/labels\"\n",
      "    noisy_patch_dataset = CenterlineDataset(img_dir_noisy_patch, label_dir_noisy_patch, transform, augmentation=False)\n",
      "\n",
      "\n",
      "    # combined_dataset = ConcatDataset([original_dataset, flipped_dataset, masked_dataset, masked_dataset_flipped, bg_dataset, branch_dataset, branch_dataset_flipped])\n",
      "    combined_dataset = ConcatDataset([simple_structure_dilated_dataset, \n",
      "                                      flipped_simple_structure_dilated_dataset, \n",
      "                                      simple_structure_dilated_masked_dataset, \n",
      "                                      flipped_simple_structure_dilated_masked_dataset, \n",
      "                                      branch_point_dilated_dataset, \n",
      "                                      flipped_branch_point_dilated_dataset, \n",
      "                                      branch_point_dilated_masked_dataset, \n",
      "                                      flipped_branch_point_dilated_masked_dataset, \n",
      "                                      noisy_patch_dataset])\n",
      "\n",
      "    # Determine lengths for train and test sets\n",
      "    total_size = len(combined_dataset)\n",
      "    train_size = int(0.98 * total_size)\n",
      "    test_size = total_size - train_size\n",
      "\n",
      "    # Split the dataset\n",
      "    torch.manual_seed(42)\n",
      "    train_dataset, test_dataset = random_split(combined_dataset, [train_size, test_size])\n",
      "\n",
      "    # Create data loaders\n",
      "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
      "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
      "\n",
      "    print(f\"Dataset Preparation Complete! Train loader size: {len(train_loader)}, Test loader size: {len(test_loader)}\")\n",
      "    log_messages.append(f\"Dataset Preparation Complete! Train loader size: {len(train_loader)}, Test loader size: {len(test_loader)}\")\n",
      "\n",
      "\n",
      "    # TRAINING\n",
      "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
      "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
      "    # custom_loss = WeightedBCELoss(args.custom_weight, args.loss_function) if args.custom_loss else None\n",
      "    # custom_weight = args.custom_weight if args.custom_weight else None\n",
      "    # custom_loss = WeightedBCELoss(custom_weight=custom_weight) if args.custom_weight else WeightedBCELoss()\n",
      "    custom_loss = WeightedBCELoss(weight=args.weight) if args.custom_weight else WeightedBCELoss()\n",
      "\n",
      "    model = GaborUNet(kernel_size=args.gabor_kernel_size, in_channels=1, out_channels=1, num_orientations=8, num_scales=5).to(device)\n",
      "    # model = UNet(in_channels=1, out_channels=1).to(device)\n",
      "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
      "    if args.train_from_checkpoint:\n",
      "        print(f\"Resuming training from checkpoint {args.train_from_checkpoint}\")\n",
      "        log_messages.append(f\"Resuming training from checkpoint {args.train_from_checkpoint}\")\n",
      "    else:\n",
      "        print(\"Training Started!\")\n",
      "        log_messages.append(\"Training Started!\")\n",
      "\n",
      "    checkpoint_dir = f'checkpoints-{log_id}'\n",
      "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
      "\n",
      "    # Load model from checkpoint if available\n",
      "    model, optimizer, start_epoch = load_checkpoint(checkpoint_dir, model, optimizer, device)\n",
      "\n",
      "    train_losses = []\n",
      "    test_losses = []\n",
      "    test_accuracies = []\n",
      "    f1_scores = []\n",
      "\n",
      "    # dealing with checkpoint, append logs to the log file if loading from checkpoint\n",
      "    log_file_mode = 'a' if os.path.exists(log_name) and args.train_from_checkpoint else 'w'\n",
      "    with open(log_name, log_file_mode) as log_file:\n",
      "        for message in log_messages:\n",
      "            log_file.write(message + '\n",
      "')\n",
      "\n",
      "    log_messages = []\n",
      "\n",
      "    for epoch in range(start_epoch, args.epochs + 1):\n",
      "        train_loss = train(args, model, device, train_loader, optimizer, epoch, custom_loss=custom_loss, log_name=log_name, checkpoint_dir=checkpoint_dir)\n",
      "        test_loss, test_accuracy, test_f1_score = test(args, model, device, test_loader, epoch, custom_loss=custom_loss, log_name=log_name)\n",
      "        \n",
      "        train_losses.append(train_loss)\n",
      "        test_losses.append(test_loss)\n",
      "        test_accuracies.append(test_accuracy)\n",
      "        f1_scores.append(test_f1_score)\n",
      "        \n",
      "        scheduler.step()\n",
      "\n",
      "    # torch.save(model.state_dict(), f'gabor_unet_model_state_dict-{log_id}.pth')\n",
      "    # print(f\"Model's state_dict saved to gabor_unet_model_state_dict-{log_id}.pth\")\n",
      "    # log_messages.append(f\"Model's state_dict saved to gabor_unet_model_state_dict-{log_id}.pth\")\n",
      "\n",
      "    # torch.save(model, f'gabor_unet_model_complete-{log_id}.pth')\n",
      "    # print(f\"Entire model saved to gabor_unet_model_complete-{log_id}.pth\")\n",
      "    # log_messages.append(f\"Entire model saved to gabor_unet_model_complete-{log_id}.pth\")\n",
      "\n",
      "    torch.save(model.state_dict(), f'gabor_unet_model_state_dict-{log_id}.pth')\n",
      "    print(f\"Model's state_dict saved to gabor_unet_model_state_dict-{log_id}.pth\")\n",
      "    log_messages.append(f\"Model's state_dict saved to gabor_unet_model_state_dict-{log_id}.pth\")\n",
      "\n",
      "    torch.save(model, f'gabor_unet_model_complete-{log_id}.pth')\n",
      "    print(f\"Entire model saved to gabor_unet_model_complete-{log_id}.pth\")\n",
      "    log_messages.append(f\"Entire model saved to gabor_unet_model_complete-{log_id}.pth\")\n",
      "\n",
      "    \n",
      "    # save the image into the folder 'Analysis'\n",
      "    if not args.turn_off_analysis:\n",
      "        if not os.path.exists(\"Analysis\"):\n",
      "            os.makedirs(\"Analysis\")\n",
      "        import matplotlib.pyplot as plt\n",
      "\n",
      "        plt.figure(figsize=(10, 5))\n",
      "        plt.subplot(1, 3, 1)\n",
      "        plt.plot(range(1, args.epochs + 1), train_losses, label='Train Loss')\n",
      "        plt.plot(range(1, args.epochs + 1), test_losses, label='Test Loss')\n",
      "        plt.xlabel('Epoch')\n",
      "        plt.ylabel('Loss')\n",
      "        plt.title('Loss Curve')\n",
      "        plt.legend()\n",
      "\n",
      "        plt.subplot(1, 3, 2)\n",
      "        plt.plot(range(1, args.epochs + 1), test_accuracies, color='red', label='Test Accuracy')\n",
      "        plt.xlabel('Epoch')\n",
      "        plt.ylabel('Accuracy (%)')\n",
      "        plt.title('Accuracy Curve')\n",
      "        plt.legend()\n",
      "\n",
      "        plt.subplot(1, 3, 3)\n",
      "        plt.plot(range(1, args.epochs + 1), f1_scores, color='green', label='F1 Score')\n",
      "        plt.xlabel('Epoch')\n",
      "        plt.ylabel('F1 Score')\n",
      "        plt.title('F1 Score Curve')\n",
      "        plt.legend()\n",
      "\n",
      "        # print hyperparameters in the image \n",
      "        plt.text(0.5, 0.5, \n",
      "         ('Batch size: {}\n",
      "'\n",
      "          'Epochs: {}\n",
      "'\n",
      "          'Learning rate: {}\n",
      "'\n",
      "          'Gamma: {}\n",
      "'\n",
      "          'Step size: {}\n",
      "'\n",
      "          'Custom weight: {}\n",
      "'\n",
      "          'Weight: {}\n",
      "'\n",
      "          'Weight decay: {}\n",
      "'\n",
      "          'Gabor kernel size: {}').format(args.batch_size, args.epochs, args.lr, args.gamma, \n",
      "                                          args.step_size, args.custom_weight, args.weight, \n",
      "                                          args.weight_decay, \n",
      "                                          args.gabor_kernel_size), \n",
      "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
      "\n",
      "        \n",
      "\n",
      "\n",
      "        plt.tight_layout()\n",
      "        t = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
      "        plt.savefig(f'Analysis/loss&anloss_accuracy_curve-{t}-{log_id}.png')\n",
      "        print(f\"Loss and accuracy curve saved to Analysis/loss_accuracy_curve-{t}-{log_id}.png\")\n",
      "        log_messages.append(f\"Loss and accuracy curve saved to Analysis/loss&accuracy/loss_accuracy_curve-{t}-{log_id}.png\")\n",
      "    \n",
      "\n",
      "    print(\"Training Complete!\")\n",
      "    log_messages.append(\"Training Complete!\")\n",
      "    log_messages.append(f'Ending time: {datetime.datetime.now()}')\n",
      "\n",
      "    with open(log_name, 'a') as log_file:\n",
      "        for message in log_messages:\n",
      "            log_file.write(message + '\n",
      "')\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    main()\n",
      "\n",
      "\n",
      "    \n",
      "[Python code for the model architecture]\n",
      "\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.nn.functional as F\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class GaborUNet(nn.Module):\n",
      "    def __init__(self, kernel_size, in_channels=1, out_channels=1, num_orientations=8, num_scales=5):\n",
      "        super(GaborUNet, self).__init__()\n",
      "        self.kernel_size = kernel_size\n",
      "        self.in_channels = in_channels\n",
      "        self.out_channels = out_channels\n",
      "        self.num_orientations = num_orientations\n",
      "        self.num_scales = num_scales\n",
      "\n",
      "\n",
      "\n",
      "        # Encoder (Contracting path)\n",
      "        self.enc_conv1 = self.doubleGaborConv(in_channels, kernel_size, num_orientations, num_scales)\n",
      "        self.enc_conv2 = self.doubleConv3x3(2 * num_orientations * num_scales, 32)\n",
      "        self.enc_conv3 = self.doubleConv3x3(32, 64)\n",
      "        self.enc_conv4 = self.doubleConv3x3(64, 128)\n",
      "        self.enc_conv5 = self.doubleConv3x3(128, 256)  # New encoder layer\n",
      "\n",
      "        self.up_conv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # Adjusted for new layer\n",
      "        self.dec_conv1 = self.doubleConv3x3(256, 128)  # Adjusted for new layer\n",
      "        self.up_conv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
      "        self.dec_conv2 = self.doubleConv3x3(128, 64)\n",
      "        self.up_conv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
      "        self.dec_conv3 = self.doubleConv3x3(64, 32)\n",
      "        self.up_conv4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)  \n",
      "        self.dec_conv4 = self.doubleConv3x3(2 * num_orientations * num_scales + 16, 16) # 80 + 16\n",
      "\n",
      "        self.out_conv = nn.Conv2d(16, out_channels, kernel_size=1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # Encoder\n",
      "        enc1 = self.enc_conv1(x) \n",
      "        enc2 = self.enc_conv2(F.max_pool2d(enc1, kernel_size=2, stride=2))\n",
      "        enc3 = self.enc_conv3(F.max_pool2d(enc2, kernel_size=2, stride=2))\n",
      "        enc4 = self.enc_conv4(F.max_pool2d(enc3, kernel_size=2, stride=2))\n",
      "        enc5 = self.enc_conv5(F.max_pool2d(enc4, kernel_size=2, stride=2))\n",
      "\n",
      "        dec1 = self.up_conv1(enc5)\n",
      "        dec1 = torch.cat((dec1, enc4), dim=1)\n",
      "        dec1 = self.dec_conv1(dec1)\n",
      "\n",
      "        dec2 = self.up_conv2(dec1)\n",
      "        dec2 = torch.cat((dec2, enc3), dim=1)\n",
      "        dec2 = self.dec_conv2(dec2)\n",
      "\n",
      "        dec3 = self.up_conv3(dec2)\n",
      "        dec3 = torch.cat((dec3, enc2), dim=1)\n",
      "        dec3 = self.dec_conv3(dec3)\n",
      "\n",
      "        dec4 = self.up_conv4(dec3) \n",
      "        dec4 = torch.cat((dec4, enc1), dim=1)  # Concatenation with the first encoder layer\n",
      "        dec4 = self.dec_conv4(dec4)\n",
      "\n",
      "        out = self.out_conv(dec4)\n",
      "\n",
      "        return torch.sigmoid(out)\n",
      "\n",
      "    def doubleGaborConv(self, in_channels, kernel_size, num_orientations, num_scales):\n",
      "        return nn.Sequential(\n",
      "            GaborConv2d(in_channels, kernel_size, num_orientations, num_scales),\n",
      "            nn.BatchNorm2d(2 * num_orientations * num_scales), \n",
      "            nn.ReLU(inplace=True),\n",
      "            GaborConv2d(2 * num_orientations * num_scales, kernel_size, num_orientations, num_scales),\n",
      "            nn.BatchNorm2d(2 * num_orientations * num_scales), \n",
      "            nn.ReLU(inplace=True)\n",
      "        )\n",
      "    \n",
      "    def doubleConv3x3(self, in_channels, out_channels, dropout_rate=0.5):\n",
      "        return nn.Sequential(\n",
      "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
      "            nn.BatchNorm2d(out_channels),\n",
      "            nn.ReLU(inplace=True),\n",
      "            # nn.Dropout2d(p=dropout_rate),\n",
      "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
      "            nn.BatchNorm2d(out_channels),\n",
      "            nn.ReLU(inplace=True)\n",
      "        )\n",
      "\n",
      "\n",
      "class GaborConv2d(nn.Module):\n",
      "    def __init__(self, in_channels, kernel_size, num_orientations, num_scales):\n",
      "        super(GaborConv2d, self).__init__()\n",
      "        self.in_channels = in_channels\n",
      "        self.out_channels = 2 * num_orientations * num_scales # 80\n",
      "        self.kernel_size = kernel_size\n",
      "        self.num_orientations = num_orientations\n",
      "        self.num_scales = num_scales\n",
      "        self.padding = kernel_size // 2\n",
      "        \n",
      "\n",
      "        # Generate Gabor filter parameters\n",
      "        self.sigma, self.theta, self.Lambda, self.psi, self.gamma, self.bias = self.generate_parameters(self.out_channels // 2) # 40\n",
      "        self.filter_cos = self.whole_filter(True)\n",
      "        self.filter_sin = self.whole_filter(False)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x_cos = F.conv2d(x, self.filter_cos, padding = self.padding, bias=self.bias)\n",
      "        x_sin = F.conv2d(x, self.filter_sin, padding = self.padding, bias=self.bias)\n",
      "        return torch.cat((x_cos, x_sin), 1)\n",
      "\n",
      "    def generate_parameters(self, dim_out):\n",
      "        torch.manual_seed(1)\n",
      "        # Adjusted to initialize parameters more appropriately for Gabor filters\n",
      "        sigma = nn.Parameter(torch.rand(dim_out, 1) * 2.0 + 0.5) # Random values between 0.5 and 2.5\n",
      "        theta = nn.Parameter(torch.rand(dim_out, 1) * np.pi) # Random values between 0 and Ï€\n",
      "        Lambda = nn.Parameter(torch.rand(dim_out, 1) * 3.0 + 1.0) # Random values between 1.0 and 4.0, how Lambda is good for the detection?\n",
      "        psi = nn.Parameter(torch.rand(dim_out, 1) * 2 * np.pi) # Random values between 0 and 2Ï€\n",
      "        gamma = nn.Parameter(torch.rand(dim_out, 1) * 2.0 + 0.5) # Random values between 0.5 and 2.5\n",
      "        bias = nn.Parameter(torch.randn(dim_out)) # to avoid division by zero\n",
      "        return sigma, theta, Lambda, psi, gamma, bias\n",
      "\n",
      "\n",
      "    def whole_filter(self, cos=True):\n",
      "        # Creating a tensor to hold the Gabor filters for all orientations and scales\n",
      "        result = torch.zeros(self.num_orientations*self.num_scales, self.in_channels, self.kernel_size, self.kernel_size)\n",
      "        for i in range(self.num_orientations):\n",
      "            for j in range(self.num_scales):\n",
      "                index = i * self.num_scales + j\n",
      "                # Adjusting parameters for scale and orientation\n",
      "                sigma = self.sigma[index] * (2.1 ** j) # Adjusting sigma for scale\n",
      "                theta = self.theta[index] + i * 2 * np.pi / self.num_orientations # Adjusting theta for orientation\n",
      "                Lambda = self.Lambda[index] # Keeping Lambda constant\n",
      "                psi = self.psi[index] # Keeping psi constant\n",
      "                gamma = self.gamma[index] # Keeping gamma constant\n",
      "                # Generating the Gabor filter for each channel\n",
      "                for k in range(self.in_channels):\n",
      "                    result[index, k] = self.gabor_fn(sigma, theta, Lambda, psi, gamma, self.kernel_size, cos)\n",
      "        return nn.Parameter(result)\n",
      "\n",
      "    def gabor_fn(self, sigma, theta, Lambda, psi, gamma, kernel_size, cos=True):\n",
      "        n = kernel_size // 2\n",
      "        y, x = np.ogrid[-n:n+1, -n:n+1]\n",
      "        y = torch.FloatTensor(y)\n",
      "        x = torch.FloatTensor(x)\n",
      "\n",
      "        x_theta = x * torch.cos(theta) + y * torch.sin(theta)\n",
      "        y_theta = -x * torch.sin(theta) + y * torch.cos(theta)\n",
      "\n",
      "        if cos:\n",
      "            gb = torch.exp(-.5 * (x_theta ** 2 / sigma ** 2 + y_theta ** 2 / sigma ** 2 / gamma ** 2)) * torch.cos(2 * np.pi / Lambda * x_theta + psi)\n",
      "        else:\n",
      "            gb = torch.exp(-.5 * (x_theta ** 2 / sigma ** 2 + y_theta ** 2 / sigma ** 2 / gamma ** 2)) * torch.sin(2 * np.pi / Lambda * x_theta + psi)\n",
      "        \n",
      "        return gb\n",
      "\n",
      "\n",
      "[Python code for data preparation]\n",
      "\n",
      "from torch.utils.data import Dataset\n",
      "from PIL import Image\n",
      "import os\n",
      "from torchvision.transforms import functional as TF\n",
      "from torchvision import transforms\n",
      "import random\n",
      "random.seed(42)\n",
      "# from scipy.ndimage import binary_dilation\n",
      "import numpy as np\n",
      "import torch\n",
      "\n",
      "\n",
      "class CenterlineDataset(Dataset):\n",
      "    def __init__(self, img_dir, label_dir, transform=None, augmentation=False):\n",
      "        self.img_dir = img_dir\n",
      "        self.label_dir = label_dir\n",
      "        # self.dilated_label_dir = dilated_label_dir\n",
      "        self.transform = transform\n",
      "        # self.save_dilated = save_dilated\n",
      "        self.augmentation = augmentation\n",
      "        # self.dilation_iterations = dilation_iterations\n",
      "\n",
      "        self.images = sorted([img for img in os.listdir(img_dir) if img.endswith('.png')])\n",
      "        self.labels = sorted([label for label in os.listdir(label_dir) if label.endswith('.png')])\n",
      "        print(f\"Found {len(self.images)} images and labels in the dataset.\")\n",
      "\n",
      "        # if add_poor_quality_training:\n",
      "        #     self.poor_quality_img_dir = \"data_poor_quality/samples\"\n",
      "        #     self.poor_quality_label_dir = \"data_poor_quality/labels\"\n",
      "\n",
      "        #     self.poor_quality_images = sorted([img for img in os.listdir(self.poor_quality_img_dir) if img.endswith('.png')])\n",
      "        #     self.poor_quality_labels = sorted([label for label in os.listdir(self.poor_quality_label_dir) if label.endswith('.png')])\n",
      "\n",
      "        #     self.images += self.poor_quality_images\n",
      "        #     self.labels += self.poor_quality_labels\n",
      "\n",
      "        #     print(f\"Added {len(self.poor_quality_images)} poor quality images and labels to the dataset.\")\n",
      "        #     print(f\"Total images and labels in the dataset: {len(self.images)}\")\n",
      "\n",
      "        assert len(self.images) == len(self.labels), \"The number of images and labels do not match!\"\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.images)\n",
      "    \n",
      "    def __getitem__(self, index): # I/O Intensive\n",
      "        img_path = os.path.join(self.img_dir, self.images[index])\n",
      "        label_path = os.path.join(self.label_dir, self.labels[index])\n",
      "\n",
      "        try:\n",
      "            image = Image.open(img_path)\n",
      "            # print((np.array(image) > 255).any())\n",
      "            # image = image.convert(\"L\")  # Grayscale conversion for images. It's like we do autoscaling here.\n",
      "            # print the pixel intensity\n",
      "            # print(np.array(image))\n",
      "            # print(image.mode)\n",
      "\n",
      "            # before convert label to grayscale, make all pixels greater than 0 to 65535\n",
      "            label = Image.open(label_path)\n",
      "            # label_np = np.array(label)\n",
      "            # label_np[label_np > 0] = 65535\n",
      "            # label = Image.fromarray(label_np.astype(np.uint16))\n",
      "            # # assert all pixels are in range 0-65535\n",
      "            # assert (label_np <= 65535).all(), f\"Some pixels are greater than 65535, {label_path}\"\n",
      "            # assert (label_np >= 0).all(), f\"Some pixels are less than 0, {label_path}\"\n",
      "            \n",
      "            label_np = np.array(label)\n",
      "            label_np = (label_np > 0).astype(np.uint8) * 255  # Ensure binary labels, but converts to L mode\n",
      "            # label_np = label_np > 0\n",
      "            # # print(\"The data type of the label np is*********: \", label_np.dtype)\n",
      "            label = Image.fromarray(label_np)\n",
      "            # print(label.mode)\n",
      "\n",
      "\n",
      "            # label = label.convert(\"L\")  # Grayscale conversion for labels\n",
      "            # print(np.array(label))\n",
      "\n",
      "\n",
      "            # Crop the first row and column off, adjusting the size to 64x64\n",
      "            # Assuming the original size is 65x65, crop to get (1, 1, 65, 65)\n",
      "            image = image.crop((1, 1, 65, 65))\n",
      "            label = label.crop((1, 1, 65, 65))\n",
      "\n",
      "            # Processing the label to be binary\n",
      "            # print(np.array(label))\n",
      "            # label_np = np.array(label) > 0  # Ensuring binary values, shape: (64, 64), True or False\n",
      "            # label = Image.fromarray(label_np.astype(np.uint8))  # Convert back to image\n",
      "            # print(np.array(label))\n",
      "\n",
      "            # Dilate the label \n",
      "            # label_np = np.array(label) > 0 \n",
      "            # dilated_label_np = binary_dilation(label_np, iterations=self.dilation_iterations).astype(np.uint8)\n",
      "            # dilated_label = Image.fromarray(dilated_label_np) \n",
      "\n",
      "            # save the dilated label for debugging\n",
      "            # if self.save_dilated:\n",
      "            #     if not os.path.exists(self.dilated_label_dir):\n",
      "            #         os.makedirs(self.dilated_label_dir)\n",
      "            #     dilated_label_path = os.path.join(self.dilated_label_dir, f\"dilated_label_{index}.png\")\n",
      "            #     dilated_label.save(dilated_label_path)\n",
      "\n",
      "        except Exception as e:\n",
      "            print(f\"Error opening image or label at index {index}: {e}\")\n",
      "            return None, None, None\n",
      "\n",
      "        # Always apply basic transformations if provided, transformed to a tensor\n",
      "        if self.transform is not None:\n",
      "            image = self.transform(image)\n",
      "            label = transforms.ToTensor()(label) # ToTensor here works well for 8 bit\n",
      "            # print(torch.max(label), torch.min(label))\n",
      "            # label = self.transform(label)\n",
      "            # dilated_label = self.transform(dilated_label)\n",
      "            \n",
      "        # image = torch.from_numpy(np.array(image, dtype=np.float32))\n",
      "        # label = torch.from_numpy(np.array(label, dtype=np.float32))\n",
      "        \n",
      "        # if image.dim() == 2:\n",
      "        #     image = image.unsqueeze(0)\n",
      "        # if label.dim() == 2:\n",
      "        #     label = label.unsqueeze(0)\n",
      "\n",
      "        # image_min = torch.min(image)\n",
      "        # # print(\"The minimum pixel intensity of the image is: \", image_min)\n",
      "        # image_max = torch.max(image)\n",
      "        # # print(\"The maximum pixel intensity of the image is: \", image_max)\n",
      "        # image = (image - image_min) / (image_max - image_min)\n",
      "        # # print(\"The data type of the image is: \", image.dtype)\n",
      "        \n",
      "        # label_min = torch.min(label)\n",
      "        # # print(\"The minimum pixel intensity of the label is: \", label_min)\n",
      "        # label_max = torch.max(label)\n",
      "        # # print(\"The maximum pixel intensity of the label is: \", label_max)\n",
      "        # label = (label - label_min) / (label_max - label_min)\n",
      "        # # print(\"The data type of the label is: \", label.dtype)\n",
      "                \n",
      "        \n",
      "                \n",
      "        \n",
      "\n",
      "        if self.augmentation:\n",
      "            if self.augmentation == \"rotate\":\n",
      "                angle = random.uniform(-180, 180)\n",
      "                image = TF.rotate(image, angle)\n",
      "                label = TF.rotate(label, angle)\n",
      "                # dilated_label = TF.rotate(dilated_label, angle)\n",
      "            if self.augmentation == \"flip\":\n",
      "                if random.random() > 0.5: # Horizontal flip\n",
      "                    image = TF.hflip(image)\n",
      "                    label = TF.hflip(label)\n",
      "                    # dilated_label = TF.hflip(dilated_label)\n",
      "                # if random.random() > 0.5: # Vertical flip\n",
      "                else:\n",
      "                    image = TF.vflip(image)\n",
      "                    label = TF.vflip(label)\n",
      "                    # dilated_label = TF.vflip(dilated_label)\n",
      "        \n",
      "\n",
      "        return image, label\n",
      "\n",
      "\n",
      "[Python code for inference, image reconstruction because we cut a big image into small patches and do inference on them]\n",
      "\n",
      "\n",
      "\n",
      "class Normalize16BitRange(transforms.ToTensor):\n",
      "    def __call__(self, pic):\n",
      "        '''\n",
      "            Input: PIL Image or numpy array\n",
      "            Output: Tensor Normalized to [0, 1] based on the actual data range\n",
      "        '''\n",
      "        img = torch.from_numpy(np.array(pic, dtype=np.float32, copy=True))\n",
      "        \n",
      "        # mimic ToTensor having shape [C, H, W]\n",
      "        if img.dim() == 2:\n",
      "            img = img.unsqueeze(0)\n",
      "\n",
      "        # Normalization based on the actual data range\n",
      "        # min_val = torch.min(img)\n",
      "        # max_val = torch.max(img)\n",
      "        # min_val = 108\n",
      "        max_val = 2000\n",
      "        # pixels greater than 2826 fixed at 2826\n",
      "        img = torch.where(img > max_val, torch.tensor(max_val), img)\n",
      "        # pixels less than 90 fixed at 90\n",
      "        # img = torch.where(img < min_val, torch.tensor(min_val), img)\n",
      "        img = img / max_val  # Normalize to [0, 1]\n",
      "\n",
      "        return img\n",
      "\n",
      "\n",
      "# Patch The Input Image\n",
      "\n",
      "\n",
      "def image_to_patches(image, patch_size=64, overlap=32):\n",
      "\n",
      "    #- Splits the image into patches, with optional overlap.\n",
      "    #- Input: PIL Image, uint16 \n",
      "    #- Output: Numpy array of patches and their center coordinates\n",
      "\n",
      "    patch_tuples = []\n",
      "    patches = []\n",
      "    img_np = np.array(image)\n",
      "    stride = patch_size - overlap\n",
      "    index = 0\n",
      "    for y in range(0, img_np.shape[0], stride):\n",
      "        for x in range(0, img_np.shape[1], stride):\n",
      "            center_x = x + patch_size // 2\n",
      "            center_y = y + patch_size // 2\n",
      "            \n",
      "            if y + patch_size <= img_np.shape[0] and x + patch_size <= img_np.shape[1]:\n",
      "                patch = img_np[y:y+patch_size, x:x+patch_size]\n",
      "                patches.append(patch)\n",
      "                patch_tuples.append((index, (center_x, center_y), patch))\n",
      "            else:  # Handle edges by padding\n",
      "                patch = np.pad(img_np[y:min(y+patch_size, img_np.shape[0]), x:min(x+patch_size, img_np.shape[1])],\n",
      "                               ((0, patch_size - (min(y + patch_size, img_np.shape[0]) - y)),\n",
      "                                (0, patch_size - (min(x + patch_size, img_np.shape[1]) - x))),\n",
      "                               mode='constant', constant_values=0)\n",
      "                patches.append(patch)\n",
      "                patch_tuples.append((index, (center_x, center_y), patch))\n",
      "            index += 1\n",
      "    return np.array(patches), patch_tuples\n",
      "\n",
      "\n",
      "def model_predict_patches_raw(patches, model):\n",
      "    model.eval()\n",
      "    predictions = []\n",
      "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
      "    model = model.to(device)\n",
      "\n",
      "    for patch in patches:\n",
      "        patch_tensor = Normalize16BitRange()(patch).unsqueeze(0)\n",
      "        patch_tensor = patch_tensor.to(device)\n",
      "        with torch.no_grad():\n",
      "            pred = model(patch_tensor)\n",
      "        pred = pred.cpu().numpy().squeeze(0).squeeze(0)\n",
      "        predictions.append(pred)\n",
      "    return np.array(predictions)\n",
      " \n",
      "\n",
      "def patches_to_image_raw(patches, image_size, patch_size=64, overlap=32):\n",
      "    stride = patch_size - overlap\n",
      "    reconstructed_image = np.zeros(image_size)\n",
      "    count_matrix = np.zeros(image_size)\n",
      "    patch_idx = 0\n",
      "\n",
      "    for y in range(0, image_size[0], stride):\n",
      "        for x in range(0, image_size[1], stride):\n",
      "            # Calculate boundaries for patch application\n",
      "            end_y = min(y + patch_size, image_size[0])\n",
      "            end_x = min(x + patch_size, image_size[1])\n",
      "            # Calculate the actual height and width to be used from the patch\n",
      "            patch_height = end_y - y\n",
      "            patch_width = end_x - x\n",
      "            reconstructed_image[y:end_y, x:end_x] += patches[patch_idx][:patch_height, :patch_width]\n",
      "            count_matrix[y:end_y, x:end_x] += 1\n",
      "            patch_idx += 1\n",
      "    # Avoid division by zero\n",
      "    count_matrix[count_matrix == 0] = 1\n",
      "    reconstructed_image /= count_matrix\n",
      "\n",
      "    reconstructed_image[reconstructed_image > 0.5] = 255\n",
      "    reconstructed_image[reconstructed_image <= 0.5] = 0\n",
      "    show_image(reconstructed_image)\n",
      "    return reconstructed_image\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Write an summary on the code provided, which is a segmentation model used to detect centerlines of dendrites under microscope\\n {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = \"\"\"\n",
    "Centerline Detection of Neuronal Dendrites: Deep Learning Model Workflow\n",
    "Chainathan Santhanam Sudhakar 24 May 2024\n",
    "1 Introduction\n",
    "The objective of this project is to develop a model for centerline detection of Neuronal Dendrite mem- branes. The goal is to predict the â€™backboneâ€™ of the structure with outputs that are as continuous as possible. This task falls under the category of binary segmentation, focusing on identifying the central lines within membrane structures in grayscale images.\n",
    "The provided dataset consists of 16-bit grayscale images of size 65x65 pixels. The membrane struc- tures have intensity values ranging from approximately 150 to 1000, whereas the background intensity ranges from approximately 110 to 150. However, it is noted that in some areas, the noise pixel values are similar to those of the membranes, which presents an additional challenge for accurate segmentation.\n",
    "The model needs to handle these high-depth images and accurately segment the central lines of the membranes despite the presence of noise.\n",
    "2 Data Preparation 2.1 Data Collection\n",
    "The data used for this task was provided by evaluators, consisting of 28,634 samples stored as 65x65 16-bit PNG images. Each image is grayscale, with corresponding segmentation masks also stored as 65x65 PNGs. This ensures a direct mapping between images and their labels.\n",
    "2.2 Data Preprocessing\n",
    "Effective data preprocessing is critical for preparing the data for model training. The following steps were taken:\n",
    "1. Intensity Range Adjustment:\n",
    "â€¢ From the data analysis, the intensity range for the background was adjusted from [110, 150] to [100, 150], and for the membrane from [150, 1000] to [150, 1010]. This adjustment ensures a higher percentage of samples fall within these ranges, thereby improving the robustness of the model.\n",
    "2. Handling Empty Samples:\n",
    "â€¢ It was observed that 1,000 samples (3.5% of the total dataset) contained no centerline informa- tion. These empty samples were reduced to 100, as retaining a small number of empty samples (0.35% of the total dataset) helps the model learn to identify the absence of centerlines, which is a valid scenario. However, having too many could bias the model towards predicting no centerline, which would be detrimental to its performance on samples with actual centerlines.\n",
    "3. Normalization and Clipping:\n",
    "â€¢ Images were clipped to the specified ranges and normalized to the [0, 1] range using MINMAX Normalization. This step is crucial for ensuring that the pixel values are within a consistent range for model training, enhancing convergence during the training process.\n",
    "4. Data Augmentation:\n",
    "1\n",
    "â€¢ Applied transformations to increase the diversity of the training data and improve the modelâ€™s ability to generalize to new, unseen data:\n",
    "â€“ Elastic Deformation: These deformations simulate realistic distortions that could occur in the membrane images, helping the model become robust to slight variations and dis- tortions. Elastic deformation involves randomly distorting the image and mask together, maintaining their alignment.\n",
    "â€“ Horizontal and Vertical Flips: These augmentations enhance the modelâ€™s ability to rec- ognize membranes irrespective of their orientation.\n",
    "Additionally, an attempt was made to use a Patch and Reconstruct approach:\n",
    "â€¢ Patch Construction: This approach involved randomly selecting four images, constructing larger patches, and using these to train the model. The images were then either downscaled to 65x65 or directly fed into the model as 110x110 images. However, the results were suboptimal. Downscaling led to significant information loss due to the small resolution, while the larger patch size might have led to difficulty in learning due to the increase in input size, potentially requiring modifications in the model architecture and hyperparameters to handle effectively.\n",
    "5. Dataset and DataLoader Construction:\n",
    "â€¢ Custom PyTorch Dataset classes (CenterlineDataset and PatchConstructDataset) were im- plemented to handle the preprocessing steps and facilitate loading of data during training. The DataLoader objects were created for batching and shuffling the data during training and evaluation phases.\n",
    "2.3 Data Splitting\n",
    "The dataset was split into training, validation, and test sets with a ratio of 70% training, 15% validation, and 15% testing. This ensures that the model is trained on a substantial portion of the data while reserving enough data for unbiased validation and testing.\n",
    "3 Model Development 3.1 Model Selection\n",
    "For the task of centerline detection of membranes, the U-Net architecture was selected due to its ef- fectiveness in biomedical image segmentation tasks. U-Net is particularly well-suited for this task as it captures both the spatial context and fine-grained details, which are essential for accurate segmentation.\n",
    "Additionally, several variations of U-Net were experimented with to enhance performance:\n",
    "1. Standard U-Net: A custom baseline model for our task based on Original U-Net architecture.\n",
    "2. Mini U-Net: A smaller version to test the trade-off between model complexity and performance.\n",
    "3. Extended U-Net: An extended version with additional layers to improve noise handling and segmentation accuracy.\n",
    "4. U-Net with Self-Attention (U-Net SA): Incorporates self-attention mechanisms to better capture long-range dependencies and improve segmentation quality.\n",
    "5. Mini U-Net with Self-Attention: A smaller version of U-Net SA.\n",
    "3.2 Model Architecture\n",
    "1. Standard U-Net:\n",
    "â€¢ Size: 1931201\n",
    "â€¢ Encoding Path: Consists of three encoding blocks, each followed by a max-pooling layer. Each block includes two convolutional layers with ReLU activations and batch normalization, designed to progressively capture more complex features.\n",
    "2\n",
    "\n",
    "â€¢ Bottleneck: A convolutional block at the lowest resolution to capture the most abstract fea- tures of the input image.\n",
    "â€¢ Decoding Path: Symmetrically structured like the encoding path, but with transposed con- volutions (upconvolutions) for upsampling. Each decoding block combines features from the corresponding encoding block through concatenation, followed by convolutional layers.\n",
    "â€¢ Output Layer: A series of convolutional layers with ReLU activations and batch normalization, ending with a sigmoid activation to produce the final segmentation map.\n",
    "2. Mini U-Net:\n",
    "â€¢ Size: 471361\n",
    "â€¢ Encoding Path: Similar to the standard U-Net but with only two encoding blocks, reducing the modelâ€™s complexity and computational requirements.\n",
    "â€¢ Bottleneck and Decoding Path: Follows the same principles as the standard U-Net but with fewer layers.\n",
    "â€¢ Output Layer: Matches the structure of the standard U-Netâ€™s output layer. 3. Extended U-Net:\n",
    "â€¢ Size: 1933577\n",
    "â€¢ Encoding and Decoding Paths: Same structure as the standard U-Net but with additional\n",
    "layers in the output segment to enhance feature extraction and noise reduction.\n",
    "â€¢ Output Layer: Contains more convolutional layers with batch normalization and ReLU acti- vations, aiming to refine the segmentation output and mitigate the influence of noise.\n",
    "4. U-Net with Self-Attention (U-Net SA):\n",
    "â€¢ Size: 1958364\n",
    "â€¢ Encoding Path: Same structure as the standart U-Net but includes self-attention modules in the skip connection block to capture global contextual information.\n",
    "â€¢ Decoding Path: Similar to the standard U-Net but incorporates the enhanced feature maps from the self-attention modules.\n",
    "â€¢ Output Layer: Structured like the standard U-Net, ending with a sigmoid activation for the final output.\n",
    "5. U-Net Mini with Self-Attention (U-Net Mini SA):\n",
    "â€¢ Size: 477883\n",
    "â€¢ Encoding Path: Similar to the standard U-Net with Self Attention but with only two encoding blocks, reducing the modelâ€™s complexity and computational requirements.\n",
    "3.3 Training\n",
    "3.3.1 Training Setup 1. Hyperparameters:\n",
    "â€¢ Learning Rate: Initially set to 1e-3 and then adjusted towards 1e-4 based on the convergence behavior.\n",
    "â€¢ Batch Size: Depending on the available computational resources (the models were trained between Kaggle and local GPU based on availability), a batch size of 128(64 on local GPU) was chosen to balance between training speed and stability.\n",
    "â€¢ Number of Epochs: Set to 50 epochs, or until convergence. 2. Loss Function:\n",
    "â€¢ Binary Cross-Entropy Loss: Given the binary nature of the segmentation task, BCE is used as it measures the difference between the predicted probability distribution and the actual distribution. It is particularly suitable for pixel-wise classification tasks like segmentation.\n",
    "3\n",
    "\n",
    "â€¢ Combined: Dice Loss + BCE Loss; Additionally, Dice Loss is used to handle class imbalance and ensure that the overlap between the predicted and actual segments is maximized. The combined loss was be used to leverage the strengths of both.\n",
    "3. Optimization Algorithm:\n",
    "â€¢ Adam Optimizer: Selected for its efficiency and adaptive learning rate capabilities. Adam combines the advantages of AdaGrad and RMSProp, making it well-suited for dealing with sparse gradients and noisy data.\n",
    "4. Training Loop:\n",
    "â€¢ Forward Pass: Compute the predicted segmentation mask for the input batch.\n",
    "â€¢ Loss Calculation: Calculate the BCE or Combined Loss for the batch.\n",
    "â€¢ Backward Pass: Perform backpropagation to compute the gradients.\n",
    "â€¢ Weight Update: Update the model weights using the Adam optimizer.\n",
    "â€¢ Monitoring: Track training and validation loss and metrics at the end of each epoch to monitor progress.\n",
    "3.4 Testing\n",
    "3.4.1 Model Evaluation 1. Performance Metrics:\n",
    "â€¢ Intersection over Union (IoU): Measures the overlap between the predicted segmentation and the ground truth, divided by the union of both. It is a critical metric for segmentation tasks, especially for evaluating the accuracy of the predicted regions.\n",
    "â€¢ Dice Coefficient: Similar to IoU, it measures the overlap between the predicted and actual segments but is more sensitive to small segmentations, making it useful for medical image segmentation where precision is crucial.\n",
    "â€¢ Precision and Recall: Precision measures the ratio of true positive predictions to all positive predictions, while recall measures the ratio of true positive predictions to all actual positives. These metrics are important for understanding the balance between false positives and false negatives.\n",
    "â€¢ F1-Score: The harmonic mean of precision and recall, providing a single metric that balances both aspects. (Primary Focus)\n",
    "2. Evaluation Procedure:\n",
    "â€¢ Loading the Test Data: Use the test set created during the data preparation phase. Ensure\n",
    "that the data is not used during training to maintain an unbiased evaluation.\n",
    "â€¢ Model Inference: Run the trained model on the test set to obtain predicted segmentation masks.\n",
    "â€¢ Metric Calculation: Compute the performance metrics for the predicted masks against the ground truth masks.\n",
    "3. Visualization:\n",
    "â€¢ Segmentation Results: Visualize a few test images with their predicted and ground truth\n",
    "segmentation masks to qualitatively assess the modelâ€™s performance.\n",
    "â€¢ Metric Visualization: Plot metrics for the test set to provide a clear understanding of the modelâ€™s performance.\n",
    "4\n",
    "\n",
    "4 Inference\n",
    "4.1 Loading the Trained Model\n",
    "1. Model Loading:\n",
    "â€¢ Load the trained model weights from the saved checkpoint to ensure that the model can be used for inference on new data. This involves initializing the model architecture and loading the state dictionary with the trained weights and moving the model to respective device.\n",
    "4.2 Running Inference on New Data\n",
    "1. Data Preparation:\n",
    "â€¢ Loading Images: Load the new 16-bit grayscale images.\n",
    "â€¢ Clipping Intensity Values: Clip the intensity values of the images to the range [100, 1010].\n",
    "â€¢ Normalization: Normalize the images to the [0, 1] range.\n",
    "â€¢ Adding Channel Dimension: Add a channel dimension to the images to match the input requirements of the model.\n",
    "2. Inference Process:\n",
    "â€¢ Pass the new data through the loaded model to obtain the predicted segmentation masks.\n",
    "â€¢ Ensure the model is in evaluation mode to disable batch normalization layers from updating during inference.\n",
    "3. Post-processing:\n",
    "â€¢ Apply post-processing steps to refine the predicted masks. This includes thresholding the output probabilities to obtain binary masks and applying morphological operations like skele- tonization depending on the task requirement.\n",
    "4.3 Visualization of Inferred Segmentation Masks\n",
    "1. Displaying Results:\n",
    "â€¢ Visualize the original images alongside the predicted segmentation masks to qualitatively assess the modelâ€™s performance on new data. This helps in understanding how well the model generalizes to unseen samples.\n",
    "2. Saving Results:\n",
    "â€¢ Save the predicted masks for further analysis or for use in downstream applications. This can\n",
    "be done by converting the predicted tensors to images and saving them in the desired format.\n",
    "5\n",
    "\n",
    "5 Results\n",
    "The models were evaluated based on several metrics, with a particular focus its balance between precision and recall. Below are the results of the different configurations, listed in order of their F1 scores:\n",
    "on the F1 score due to model architectures and\n",
    " Model\n",
    "Normal U-Net Mini U-Net Mini SA U-Net Extended U-Net SA U-Net\n",
    "Epoch Loss\n",
    "42 BCE\n",
    "43 BCE\n",
    "19 BCE\n",
    "55 Combined 40 BCE\n",
    "F1 Score\n",
    "0.9532 0.9409 0.9334 0.9425 0.9249\n",
    "Dice IoU Precision\n",
    "0.9532 0.9106 0.9687 0.9409 0.8884 0.9644 0.9334 0.8754 0.9552 0.9425 0.8914 0.9235 0.9249 0.8606 0.9487\n",
    "Recall\n",
    "0.9381 0.9185 0.9128 0.9624 0.9025\n",
    "      6\n",
    "â€¢\n",
    "â€¢\n",
    "â€¢\n",
    "â€¢\n",
    "â€¢\n",
    "Table 1: Best Performance metrics of different U-Net models.\n",
    "Detailed Results and Analysis\n",
    "Normal U-Net with BCE Loss: Achieved the highest F1 score, indicating the best overall performance in balancing precision and recall. This model is particularly effective in accurately identifying both true positive and false negative rates.\n",
    "Mini U-Net with BCE Loss: While being a smaller model, it still provided a high F1 score, demonstrating that reducing the model complexity did not significantly compromise its ability to accurately segment the images.\n",
    "Mini Self-Attention U-Net with BCE Loss: The inclusion of self-attention mechanisms in the mini U-Net improved its ability to understand spatial dependencies, leading to a robust seg- mentation performance with a high F1 score with least amount of training.\n",
    "Extended U-Net with Combined Loss: This model had a high recall, making it very effective in identifying true positive segments. The use of Dice Loss helped in balancing class imbalances but resulted in slightly lower precision.\n",
    "Self-Attention U-Net with BCE Loss: Despite incorporating self-attention, this model had a lower recall compared to the other models, which impacted its overall F1 score. However, it still showed good precision.\n",
    "The analysis of these results indicates that the Normal U-Net with BCE Loss is the best performing model for this task, achieving the highest F1 score and thus the best balance between precision and recall. Other models also performed well, each with its strengths, suggesting various trade-offs between complexity, precision, and recall.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "\n",
    "[Python code for training the model]\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from data import CenterlineDataset\n",
    "from gabor_unet_nfc import GaborUNet\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader, random_split, ConcatDataset\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import os\n",
    "import datetime\n",
    "import uuid\n",
    "import math\n",
    "from scipy.ndimage import distance_transform_edt\n",
    "import numpy as np\n",
    "from unet_model import UNet\n",
    "import torch\n",
    "from torchvision.transforms import functional as TF\n",
    "\n",
    "\n",
    "class To16BitTensor:\n",
    "    def __call__(self, image):\n",
    "        image_tensor = TF.to_tensor(image)\n",
    "        image_tensor = image_tensor / 65535.0\n",
    "        return image_tensor\n",
    "\n",
    "class Normalize16BitRange(transforms.ToTensor):\n",
    "    def __call__(self, pic):\n",
    "        '''\n",
    "            Input: PIL Image or numpy array\n",
    "            Output: Tensor Normalized to [0, 1] based on the actual data range\n",
    "        '''\n",
    "        img = torch.from_numpy(np.array(pic, dtype=np.float32, copy=True))\n",
    "        \n",
    "        # mimic ToTensor having shape [C, H, W]\n",
    "        if img.dim() == 2:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Normalization based on the actual data range\n",
    "        # min_val = torch.min(img)\n",
    "        # max_val = torch.max(img)\n",
    "        # min_val = 108\n",
    "        max_val = 2000\n",
    "        # pixels greater than 2826 fixed at 2826\n",
    "        img = torch.where(img > max_val, torch.tensor(max_val), img)\n",
    "        # pixels less than 90 fixed at 90\n",
    "        # img = torch.where(img < min_val, torch.tensor(min_val), img)\n",
    "        img = img / max_val  # Normalize to [0, 1]\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "       \n",
    "class WeightedBCELoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "        self.custom_weight = weight\n",
    "    def forward(self, prediction, label):\n",
    "        \n",
    "        # Apply the general weighted binary cross entropy loss function, assigning higher penality to false negatives.\n",
    "        # The purpose here is to handle imbalanced datasets, where the number of negative pixels is much higher than the number of positive pixels. \n",
    "        \n",
    "        weight = torch.ones_like(label)\n",
    "        # Handle class imbalance\n",
    "        # GET THE WEIGHT\n",
    "        if self.custom_weight is not None:\n",
    "            weight[(label == 1)] = self.custom_weight\n",
    "        else:        \n",
    "            # calculate the number of all pixles in the label \n",
    "            total_pixels = label.numel()\n",
    "            # calculate the number of positive pixels in the label\n",
    "            positive_pixels = label.sum().item()\n",
    "            # calculate the number of negative pixels in the label\n",
    "            negative_pixels = total_pixels - positive_pixels\n",
    "            # calculate the weights for the positive pixels and negative pixels\n",
    "            weight_positive_prime = max(math.log(2 * total_pixels / (positive_pixels + 1e-6)), 1.0)\n",
    "            weight_negative_prime = max(math.log(2 * total_pixels / (negative_pixels+ 1e-6)), 1.0)\n",
    "            weight_positive = weight_positive_prime / (weight_positive_prime + weight_negative_prime)\n",
    "            weight_negative = weight_negative_prime / (weight_positive_prime + weight_negative_prime)\n",
    "            # assign the weights to the pixels\n",
    "            weight[label == 1] = weight_positive\n",
    "            weight[label == 0] = weight_negative\n",
    "        \n",
    "            # Assign significance per pixel based on their location\n",
    "            # find the distance from each pixel to the nearest centerline pixel\n",
    "            # label_np = label.numpy()\n",
    "            # binary_mask = (label_np == 0).astype(np.int) # 1 for background, 0 for centerline\n",
    "            # distance_map_np = distance_transform_edt(binary_mask)\n",
    "            # distance_map = torch.from_numpy(distance_map_np)\n",
    "            # GPU compatible version\n",
    "            label_np = label.cpu().numpy()\n",
    "            binary_mask = (label_np == 0).astype(int) # 1 for background, 0 for centerline\n",
    "            distance_map_np = distance_transform_edt(binary_mask)\n",
    "            distance_map = torch.from_numpy(distance_map_np).to(label.device)\n",
    "            weight = weight * (1 - 0.01) ** distance_map\n",
    "        \n",
    "        # print(\"The data type of prediction is: \", prediction.dtype)\n",
    "        # print(\"The data type of label is: \", label.dtype)\n",
    "\n",
    "        loss = F.binary_cross_entropy(prediction, label, weight=weight) \n",
    "        return loss\n",
    "\n",
    "\n",
    "def save_checkpoint(state, filename='checkpoint.pth.tar'):\n",
    "    # state here is a dictionary containing the model's state_dict, the optimizer's state_dict, and the epoch number\n",
    "    torch.save(state, filename)\n",
    "    print(f\"Checkpoint saved to {filename}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_dir, model, optimizer, device):\n",
    "    latest_checkpoint = None\n",
    "    max_epoch = -1\n",
    "    for file in os.listdir(checkpoint_dir):\n",
    "        if file.startswith('checkpoint') and file.endswith('.pth.tar'):\n",
    "            epoch_num = int(file.split('_')[-1].split('.')[0])\n",
    "            if epoch_num > max_epoch:\n",
    "                max_epoch = epoch_num\n",
    "                latest_checkpoint = file\n",
    "    \n",
    "    if latest_checkpoint is not None:\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, latest_checkpoint)\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        print(f\"Checkpoint loaded. Starting from epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "        start_epoch = 1\n",
    "    \n",
    "    return model, optimizer, start_epoch\n",
    "\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch, custom_loss=None, log_name='Analysis/logs/log.txt', checkpoint_dir = 'checkpoints'):\n",
    "    log_messages = []\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data_tuple in enumerate(train_loader):\n",
    "        # Unpack the data_tuple and check for None values\n",
    "        if any(x is None for x in data_tuple):\n",
    "            print(f\"Skipping batch {batch_idx} due to None values\")\n",
    "            log_messages.append(f\"Skipping batch {batch_idx} due to None values\")\n",
    "            continue  # Skip this batch\n",
    "        \n",
    "        data, target = (x.to(device) for x in data_tuple)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # print(target.shape)\n",
    "        # print(output.shape)\n",
    "        # print(target.dtype)\n",
    "        # print(output.dtype)\n",
    "        # print(\"The max value of the target is: \", torch.max(target).item())\n",
    "        # print(\"The min value of the target is: \", torch.min(target).item())\n",
    "        # print(\"The max value of the output is: \", torch.max(output).item())\n",
    "        # print(\"The min value of the output is: \", torch.min(output).item())\n",
    "        if not custom_loss:\n",
    "            loss = F.binary_cross_entropy(output, target)\n",
    "        else:\n",
    "            # loss = custom_loss(output, target, dilated_target)\n",
    "            loss = custom_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            message = f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss (Batch): {loss.item():.6f}'\n",
    "            print(message)\n",
    "            # add to log \n",
    "            log_messages.append(message)\n",
    "\n",
    "    with open(log_name, 'a') as log_file:\n",
    "        for message in log_messages:\n",
    "            log_file.write(f\"{'*'*20} Training Epoch: {epoch} {'*'*20}\\n\")\n",
    "            log_file.write(message + '\\n')\n",
    "\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f\"checkpoint_epoch_{epoch}.pth.tar\")\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'optimizer': optimizer.state_dict(),\n",
    "    }, filename=checkpoint_path)\n",
    "\n",
    "    average_loss = train_loss / len(train_loader)\n",
    "    return average_loss\n",
    "\n",
    "\n",
    "def test(args, model, device, test_loader, epoch, custom_loss=None, log_name='Analysis/logs/log.txt', loss_function='bce'):\n",
    "    log_messages = []\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    total_correct_pixels = 0\n",
    "    total_centerline_pixels = 0\n",
    "    test_f1_score = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, data_tuple in enumerate(test_loader):\n",
    "            if any(x is None for x in data_tuple):\n",
    "                print(f\"Skipping batch {batch_idx} due to None values\")\n",
    "                log_messages.append(f\"Skipping batch {batch_idx} due to None values\")\n",
    "                continue \n",
    "            \n",
    "            data, target = (x.to(device) for x in data_tuple)\n",
    "            output = model(data)\n",
    "            if not custom_loss:\n",
    "                loss = F.binary_cross_entropy(output, target, reduction='sum').item()\n",
    "            else:\n",
    "                # loss = custom_loss(output, target, dilated_target).item()\n",
    "                loss = custom_loss(output, target).item()\n",
    "            test_loss += loss\n",
    "            # pred = output > 0.5\n",
    "            # correct_pixels = pred.eq(target.view_as(pred)).sum().item()\n",
    "            # total_correct_pixels += correct_pixels\n",
    "            # total_pixels += target.numel()\n",
    "            # accuracy = 100. * correct_pixels / target.numel()\n",
    "\n",
    "            # accuracy = the number of corrected predicted centerline pixels divided by the number of centerline pixels in the target\n",
    "            pred_binary = (output > 0.5).type(torch.bool)\n",
    "            target_binary = target.type(torch.bool)\n",
    "\n",
    "            # centerline_pixels = target_binary.sum().item()\n",
    "            # total_centerline_pixels += centerline_pixels\n",
    "            # correct_pixels = (pred_binary & target_binary).sum().item()\n",
    "            # total_correct_pixels += correct_pixels\n",
    "\n",
    "            # if centerline_pixels > 0:\n",
    "            #     accuracy = 100. * correct_pixels / centerline_pixels\n",
    "            # else:\n",
    "            #     accuracy = 100\n",
    "\n",
    "            TP = (pred_binary & target_binary).sum().item()\n",
    "            FP = (pred_binary & ~target_binary).sum().item()\n",
    "            FN = (~pred_binary & target_binary).sum().item()\n",
    "\n",
    "            # calculate accuracy, precision, and recall\n",
    "            if (TP + FN) > 0:\n",
    "                recall = 100. * TP / (TP + FN)\n",
    "            else:\n",
    "                recall = 0\n",
    "            \n",
    "            if (TP + FP) > 0:\n",
    "                precision = 100. * TP / (TP + FP)\n",
    "            else:\n",
    "                precision = 0\n",
    "            \n",
    "            # Calculate F1 Score\n",
    "            if (precision + recall) > 0:\n",
    "                f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "            else:\n",
    "                f1_score = 0\n",
    "                \n",
    "            test_f1_score += f1_score\n",
    "            total_centerline_pixels += target_binary.sum().item()\n",
    "            total_correct_pixels += TP\n",
    "\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                message = f'Test Epoch: {epoch} [{batch_idx * len(data)}/{len(test_loader.dataset)} ({100. * batch_idx / len(test_loader):.0f}%)]\\tLoss (Batch): {loss:.6f} \\t Accuracy (Batch): {recall:.2f}% \\t F1 Score (Batch): {f1_score:.2f}%'\n",
    "                print(message)\n",
    "                # add to log\n",
    "                log_messages.append(message)\n",
    "\n",
    "    with open(log_name, 'a') as log_file:\n",
    "        for message in log_messages:\n",
    "            log_file.write(f\"\\n{'*'*20} Testing Epoch: {epoch} {'*'*20}\\n\")\n",
    "            log_file.write(message + '\\n')     \n",
    "\n",
    "    average_loss = test_loss / len(test_loader)  # Calculate average loss per batch\n",
    "    average_accuracy = 100. * total_correct_pixels / total_centerline_pixels  # Calculate recall (accuracy)\n",
    "    average_f1_score = test_f1_score / len(test_loader)  # Calculate average F1 score per batch\n",
    "    return average_loss, average_accuracy, average_f1_score\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # COMMAND LINE ARGUMENTS\n",
    "    parser = argparse.ArgumentParser(description='PyTorch GaborUNet Training')\n",
    "    parser.add_argument('--batch-size', type=int, default=16, metavar='N',\n",
    "                        help='input batch size for training (default: 16)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=100, metavar='N',\n",
    "                        help='input batch size for testing (default: 100)')\n",
    "    parser.add_argument('--epochs', type=int, default=50, metavar='N',\n",
    "                        help='number of epochs to train (default: 50)')\n",
    "    parser.add_argument('--lr', type=float, default=0.01, metavar='LR',\n",
    "                        help='learning rate (default: 0.01)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--step-size', type=int, default=1, metavar='N',)\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                        help='disables CUDA training')\n",
    "    # parser.add_argument('--save-dilated', action='store_true', default=False, \n",
    "    #                     help='save dilated labels for debugging')\n",
    "    parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "    parser.add_argument('--custom-weight', action='store_true', default=False,\n",
    "                        help='less penality for pixels in dilated area but not in original target')\n",
    "    parser.add_argument('--weight', type=float, default=8, help='Custom weight for the BC or NLL loss for pixels in dilated area but not in the original target')\n",
    "    parser.add_argument('--weight-decay', type=float, default=1e-5, help='Weight decay for optimizer')\n",
    "    # parser.add_argument('--add-poor-quality-training', action='store_true', default=False, help='Add poor quality training data to the dataset')\n",
    "    parser.add_argument('--turn-off-analysis', action='store_true', default=False, help='Turn off analysis of the dataset (saving the loss image)')\n",
    "    parser.add_argument('--gabor-kernel-size', type=int, default=19, help='Size of the Gabor kernel')\n",
    "    parser.add_argument('--message', type=str, default='', help='Additional message to add to the log')\n",
    "    # parser.add_argument('--dilation-iterations', type=int, default=1, help='Number of dilation iterations for the dilated label')\n",
    "    # parser.add_argument('--loss-function' , type=str, default='bce', help='Loss function to use (bce or nll)')\n",
    "    parser.add_argument('--train-from-checkpoint', type=str, default=None, help='Input the logid of the the checkpoint to resume training')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    log_messages = []\n",
    "    # generate a unique id \n",
    "    log_id = uuid.uuid4() if args.train_from_checkpoint is None else args.train_from_checkpoint\n",
    "    log_name = f'Analysis/logs/log-{datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")}-{log_id}.txt'\n",
    "\n",
    "    # add to log\n",
    "    log_messages.append(f'Log ID: {log_id}; Message: {args.message}')\n",
    "    log_messages.append(f'Starting time: {datetime.datetime.now()}')\n",
    "    log_messages.append(f\"{'*'*20}\")\n",
    "    log_messages.append(f\"Batch size: {args.batch_size}, help='input batch size for training (default: 16)\")\n",
    "    log_messages.append(f\"Number of epochs: {args.epochs}, help='number of epochs to train (default: 50)\")\n",
    "    log_messages.append(f\"Learning rate: {args.lr}, help='learning rate (default: 0.01)\")\n",
    "    log_messages.append(f\"Learning rate step gamma: {args.gamma}, help='learning rate step gamma (default: 0.7)\")\n",
    "    log_messages.append(f\"Learning rate step size: {args.step_size}, help='learning rate step size\")\n",
    "    # log_messages.append(f\"Save dilated labels: {args.save_dilated}, help='save dilated labels for debugging\")\n",
    "    log_messages.append(f\"Log interval: {args.log_interval}, help='how many batches to wait before logging training status\")\n",
    "    log_messages.append(f\"Custom weight: {args.custom_weight}, help='less penality for pixels in dilated area but not in original target\")\n",
    "    log_messages.append(f\"weight: {args.weight}, help='Custom weight for the BCE OR NLL loss for pixels in dilated area but not in the original target\")\n",
    "    log_messages.append(f\"Weight decay: {args.weight_decay}, help='Weight decay for optimizer\")\n",
    "    # log_messages.append(f\"Add poor quality training: {args.add_poor_quality_training}, help='Add poor quality training data to the dataset\")\n",
    "    log_messages.append(f\"Turn off analysis: {args.turn_off_analysis}, help='Turn off analysis of the dataset (saving the loss image)\")\n",
    "    log_messages.append(f\"Gabor kernel size: {args.gabor_kernel_size}, help='Size of the Gabor kernel\")\n",
    "    # log_messages.append(f\"Dilation iterations: {args.dilation_iterations}, help='Number of dilation iterations for the dilated label\")\n",
    "    # log_messages.append(f\"Loss function: {args.loss_function}, help='Loss function to use (bce or nll)\")\n",
    "    log_messages.append(f\"Train from checkpoint: {args.train_from_checkpoint}, help='Input the logid of the the checkpoint to resume training\")\n",
    "    log_messages.append(f\"{'*'*20}\")\n",
    "\n",
    "\n",
    "    if args.custom_weight:\n",
    "        print(f\"Using custom loss function with weight: {args.weight}\")\n",
    "        log_messages.append(f\"Using custom loss function with weight: {args.weight}\")\n",
    "\n",
    "\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        # transforms.RandomHorizontalFlip(),  \n",
    "        # transforms.RandomRotation(15),  \n",
    "        # transforms.ToTensor(),  \n",
    "        Normalize16BitRange(),\n",
    "        # Normalize16Bit(),\n",
    "        # transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    # hand-labelled simple_structure_dataset_dilated\n",
    "    data_dir_simple_structure_dilated = 'simple_structure_dataset_dilated'\n",
    "    img_dir_simple_structure_dilated = f\"{data_dir_simple_structure_dilated}/samples\"\n",
    "    label_dir_simple_structure_dilated = f\"{data_dir_simple_structure_dilated}/labels\"\n",
    "    simple_structure_dilated_dataset = CenterlineDataset(img_dir_simple_structure_dilated, label_dir_simple_structure_dilated, transform, augmentation=False)\n",
    "    flipped_simple_structure_dilated_dataset = CenterlineDataset(img_dir_simple_structure_dilated, label_dir_simple_structure_dilated, transform, augmentation=\"flip\")\n",
    "\n",
    "\n",
    "    # hand-labelled simple_structure_dataset_dilated_masked\n",
    "    data_dir_simple_structure_dilated_masked = 'simple_structure_dataset_dilated_masked'\n",
    "    img_dir_simple_structure_dilated_masked = f\"{data_dir_simple_structure_dilated_masked}/samples\"\n",
    "    label_dir_simple_structure_dilated_masked = f\"{data_dir_simple_structure_dilated_masked}/labels\"\n",
    "    simple_structure_dilated_masked_dataset = CenterlineDataset(img_dir_simple_structure_dilated_masked, label_dir_simple_structure_dilated_masked, transform, augmentation=False)\n",
    "    flipped_simple_structure_dilated_masked_dataset = CenterlineDataset(img_dir_simple_structure_dilated_masked, label_dir_simple_structure_dilated_masked, transform, augmentation=\"flip\")\n",
    "\n",
    "    # hand-labelled branch_point_dataset_dilated\n",
    "    data_dir_branch_point_dilated = 'branch_point_dataset_dilated'\n",
    "    img_dir_branch_point_dilated = f\"{data_dir_branch_point_dilated}/samples\"\n",
    "    label_dir_branch_point_dilated = f\"{data_dir_branch_point_dilated}/labels\"\n",
    "    branch_point_dilated_dataset = CenterlineDataset(img_dir_branch_point_dilated, label_dir_branch_point_dilated, transform, augmentation=False)\n",
    "    flipped_branch_point_dilated_dataset = CenterlineDataset(img_dir_branch_point_dilated, label_dir_branch_point_dilated, transform, augmentation=\"flip\")\n",
    "\n",
    "    # hand-labelled branch_point_dataset_dilated_masked\n",
    "    data_dir_branch_point_dilated_masked = 'branch_point_dataset_dilated_masked'\n",
    "    img_dir_branch_point_dilated_masked = f\"{data_dir_branch_point_dilated_masked}/samples\"\n",
    "    label_dir_branch_point_dilated_masked = f\"{data_dir_branch_point_dilated_masked}/labels\"\n",
    "    branch_point_dilated_masked_dataset = CenterlineDataset(img_dir_branch_point_dilated_masked, label_dir_branch_point_dilated_masked, transform, augmentation=False)\n",
    "    flipped_branch_point_dilated_masked_dataset = CenterlineDataset(img_dir_branch_point_dilated_masked, label_dir_branch_point_dilated_masked, transform, augmentation=\"flip\")\n",
    "\n",
    "    # hand labelled noisy_patch_dataset\n",
    "    data_dir_noisy_patch = 'noisy_patch_dataset'\n",
    "    img_dir_noisy_patch = f\"{data_dir_noisy_patch}/samples\"\n",
    "    label_dir_noisy_patch = f\"{data_dir_noisy_patch}/labels\"\n",
    "    noisy_patch_dataset = CenterlineDataset(img_dir_noisy_patch, label_dir_noisy_patch, transform, augmentation=False)\n",
    "\n",
    "\n",
    "    # combined_dataset = ConcatDataset([original_dataset, flipped_dataset, masked_dataset, masked_dataset_flipped, bg_dataset, branch_dataset, branch_dataset_flipped])\n",
    "    combined_dataset = ConcatDataset([simple_structure_dilated_dataset, \n",
    "                                      flipped_simple_structure_dilated_dataset, \n",
    "                                      simple_structure_dilated_masked_dataset, \n",
    "                                      flipped_simple_structure_dilated_masked_dataset, \n",
    "                                      branch_point_dilated_dataset, \n",
    "                                      flipped_branch_point_dilated_dataset, \n",
    "                                      branch_point_dilated_masked_dataset, \n",
    "                                      flipped_branch_point_dilated_masked_dataset, \n",
    "                                      noisy_patch_dataset])\n",
    "\n",
    "    # Determine lengths for train and test sets\n",
    "    total_size = len(combined_dataset)\n",
    "    train_size = int(0.98 * total_size)\n",
    "    test_size = total_size - train_size\n",
    "\n",
    "    # Split the dataset\n",
    "    torch.manual_seed(42)\n",
    "    train_dataset, test_dataset = random_split(combined_dataset, [train_size, test_size])\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    print(f\"Dataset Preparation Complete! Train loader size: {len(train_loader)}, Test loader size: {len(test_loader)}\")\n",
    "    log_messages.append(f\"Dataset Preparation Complete! Train loader size: {len(train_loader)}, Test loader size: {len(test_loader)}\")\n",
    "\n",
    "\n",
    "    # TRAINING\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # custom_loss = WeightedBCELoss(args.custom_weight, args.loss_function) if args.custom_loss else None\n",
    "    # custom_weight = args.custom_weight if args.custom_weight else None\n",
    "    # custom_loss = WeightedBCELoss(custom_weight=custom_weight) if args.custom_weight else WeightedBCELoss()\n",
    "    custom_loss = WeightedBCELoss(weight=args.weight) if args.custom_weight else WeightedBCELoss()\n",
    "\n",
    "    model = GaborUNet(kernel_size=args.gabor_kernel_size, in_channels=1, out_channels=1, num_orientations=8, num_scales=5).to(device)\n",
    "    # model = UNet(in_channels=1, out_channels=1).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "    if args.train_from_checkpoint:\n",
    "        print(f\"Resuming training from checkpoint {args.train_from_checkpoint}\")\n",
    "        log_messages.append(f\"Resuming training from checkpoint {args.train_from_checkpoint}\")\n",
    "    else:\n",
    "        print(\"Training Started!\")\n",
    "        log_messages.append(\"Training Started!\")\n",
    "\n",
    "    checkpoint_dir = f'checkpoints-{log_id}'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "    # Load model from checkpoint if available\n",
    "    model, optimizer, start_epoch = load_checkpoint(checkpoint_dir, model, optimizer, device)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # dealing with checkpoint, append logs to the log file if loading from checkpoint\n",
    "    log_file_mode = 'a' if os.path.exists(log_name) and args.train_from_checkpoint else 'w'\n",
    "    with open(log_name, log_file_mode) as log_file:\n",
    "        for message in log_messages:\n",
    "            log_file.write(message + '\\n')\n",
    "\n",
    "    log_messages = []\n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs + 1):\n",
    "        train_loss = train(args, model, device, train_loader, optimizer, epoch, custom_loss=custom_loss, log_name=log_name, checkpoint_dir=checkpoint_dir)\n",
    "        test_loss, test_accuracy, test_f1_score = test(args, model, device, test_loader, epoch, custom_loss=custom_loss, log_name=log_name)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        f1_scores.append(test_f1_score)\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    # torch.save(model.state_dict(), f'gabor_unet_model_state_dict-{log_id}.pth')\n",
    "    # print(f\"Model's state_dict saved to gabor_unet_model_state_dict-{log_id}.pth\")\n",
    "    # log_messages.append(f\"Model's state_dict saved to gabor_unet_model_state_dict-{log_id}.pth\")\n",
    "\n",
    "    # torch.save(model, f'gabor_unet_model_complete-{log_id}.pth')\n",
    "    # print(f\"Entire model saved to gabor_unet_model_complete-{log_id}.pth\")\n",
    "    # log_messages.append(f\"Entire model saved to gabor_unet_model_complete-{log_id}.pth\")\n",
    "\n",
    "    torch.save(model.state_dict(), f'gabor_unet_model_state_dict-{log_id}.pth')\n",
    "    print(f\"Model's state_dict saved to gabor_unet_model_state_dict-{log_id}.pth\")\n",
    "    log_messages.append(f\"Model's state_dict saved to gabor_unet_model_state_dict-{log_id}.pth\")\n",
    "\n",
    "    torch.save(model, f'gabor_unet_model_complete-{log_id}.pth')\n",
    "    print(f\"Entire model saved to gabor_unet_model_complete-{log_id}.pth\")\n",
    "    log_messages.append(f\"Entire model saved to gabor_unet_model_complete-{log_id}.pth\")\n",
    "\n",
    "    \n",
    "    # save the image into the folder 'Analysis'\n",
    "    if not args.turn_off_analysis:\n",
    "        if not os.path.exists(\"Analysis\"):\n",
    "            os.makedirs(\"Analysis\")\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(range(1, args.epochs + 1), train_losses, label='Train Loss')\n",
    "        plt.plot(range(1, args.epochs + 1), test_losses, label='Test Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title('Loss Curve')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(range(1, args.epochs + 1), test_accuracies, color='red', label='Test Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy (%)')\n",
    "        plt.title('Accuracy Curve')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(range(1, args.epochs + 1), f1_scores, color='green', label='F1 Score')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('F1 Score')\n",
    "        plt.title('F1 Score Curve')\n",
    "        plt.legend()\n",
    "\n",
    "        # print hyperparameters in the image \n",
    "        plt.text(0.5, 0.5, \n",
    "         ('Batch size: {}\\n'\n",
    "          'Epochs: {}\\n'\n",
    "          'Learning rate: {}\\n'\n",
    "          'Gamma: {}\\n'\n",
    "          'Step size: {}\\n'\n",
    "          'Custom weight: {}\\n'\n",
    "          'Weight: {}\\n'\n",
    "          'Weight decay: {}\\n'\n",
    "          'Gabor kernel size: {}').format(args.batch_size, args.epochs, args.lr, args.gamma, \n",
    "                                          args.step_size, args.custom_weight, args.weight, \n",
    "                                          args.weight_decay, \n",
    "                                          args.gabor_kernel_size), \n",
    "         horizontalalignment='center', verticalalignment='center', transform=plt.gca().transAxes)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        plt.tight_layout()\n",
    "        t = datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "        plt.savefig(f'Analysis/loss&anloss_accuracy_curve-{t}-{log_id}.png')\n",
    "        print(f\"Loss and accuracy curve saved to Analysis/loss_accuracy_curve-{t}-{log_id}.png\")\n",
    "        log_messages.append(f\"Loss and accuracy curve saved to Analysis/loss&accuracy/loss_accuracy_curve-{t}-{log_id}.png\")\n",
    "    \n",
    "\n",
    "    print(\"Training Complete!\")\n",
    "    log_messages.append(\"Training Complete!\")\n",
    "    log_messages.append(f'Ending time: {datetime.datetime.now()}')\n",
    "\n",
    "    with open(log_name, 'a') as log_file:\n",
    "        for message in log_messages:\n",
    "            log_file.write(message + '\\n')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "\n",
    "    \n",
    "[Python code for the model architecture]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GaborUNet(nn.Module):\n",
    "    def __init__(self, kernel_size, in_channels=1, out_channels=1, num_orientations=8, num_scales=5):\n",
    "        super(GaborUNet, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_orientations = num_orientations\n",
    "        self.num_scales = num_scales\n",
    "\n",
    "        # Encoder (Contracting path)\n",
    "        self.enc_conv1 = self.doubleGaborConv(in_channels, kernel_size, num_orientations, num_scales)\n",
    "        self.enc_conv2 = self.doubleConv3x3(2 * num_orientations * num_scales, 32)\n",
    "        self.enc_conv3 = self.doubleConv3x3(32, 64)\n",
    "        self.enc_conv4 = self.doubleConv3x3(64, 128)\n",
    "        self.enc_conv5 = self.doubleConv3x3(128, 256)  # New encoder layer\n",
    "\n",
    "        self.up_conv1 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)  # Adjusted for new layer\n",
    "        self.dec_conv1 = self.doubleConv3x3(256, 128)  # Adjusted for new layer\n",
    "        self.up_conv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec_conv2 = self.doubleConv3x3(128, 64)\n",
    "        self.up_conv3 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec_conv3 = self.doubleConv3x3(64, 32)\n",
    "        self.up_conv4 = nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2)  \n",
    "        self.dec_conv4 = self.doubleConv3x3(2 * num_orientations * num_scales + 16, 16) # 80 + 16\n",
    "\n",
    "        self.out_conv = nn.Conv2d(16, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.enc_conv1(x) \n",
    "        enc2 = self.enc_conv2(F.max_pool2d(enc1, kernel_size=2, stride=2))\n",
    "        enc3 = self.enc_conv3(F.max_pool2d(enc2, kernel_size=2, stride=2))\n",
    "        enc4 = self.enc_conv4(F.max_pool2d(enc3, kernel_size=2, stride=2))\n",
    "        enc5 = self.enc_conv5(F.max_pool2d(enc4, kernel_size=2, stride=2))\n",
    "\n",
    "        dec1 = self.up_conv1(enc5)\n",
    "        dec1 = torch.cat((dec1, enc4), dim=1)\n",
    "        dec1 = self.dec_conv1(dec1)\n",
    "\n",
    "        dec2 = self.up_conv2(dec1)\n",
    "        dec2 = torch.cat((dec2, enc3), dim=1)\n",
    "        dec2 = self.dec_conv2(dec2)\n",
    "\n",
    "        dec3 = self.up_conv3(dec2)\n",
    "        dec3 = torch.cat((dec3, enc2), dim=1)\n",
    "        dec3 = self.dec_conv3(dec3)\n",
    "\n",
    "        dec4 = self.up_conv4(dec3) \n",
    "        dec4 = torch.cat((dec4, enc1), dim=1)  # Concatenation with the first encoder layer\n",
    "        dec4 = self.dec_conv4(dec4)\n",
    "\n",
    "        out = self.out_conv(dec4)\n",
    "\n",
    "        return torch.sigmoid(out)\n",
    "\n",
    "    def doubleGaborConv(self, in_channels, kernel_size, num_orientations, num_scales):\n",
    "        return nn.Sequential(\n",
    "            GaborConv2d(in_channels, kernel_size, num_orientations, num_scales),\n",
    "            nn.BatchNorm2d(2 * num_orientations * num_scales), \n",
    "            nn.ReLU(inplace=True),\n",
    "            GaborConv2d(2 * num_orientations * num_scales, kernel_size, num_orientations, num_scales),\n",
    "            nn.BatchNorm2d(2 * num_orientations * num_scales), \n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def doubleConv3x3(self, in_channels, out_channels, dropout_rate=0.5):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # nn.Dropout2d(p=dropout_rate),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "class GaborConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, kernel_size, num_orientations, num_scales):\n",
    "        super(GaborConv2d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = 2 * num_orientations * num_scales # 80\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_orientations = num_orientations\n",
    "        self.num_scales = num_scales\n",
    "        self.padding = kernel_size // 2\n",
    "        \n",
    "\n",
    "        # Generate Gabor filter parameters\n",
    "        self.sigma, self.theta, self.Lambda, self.psi, self.gamma, self.bias = self.generate_parameters(self.out_channels // 2) # 40\n",
    "        self.filter_cos = self.whole_filter(True)\n",
    "        self.filter_sin = self.whole_filter(False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cos = F.conv2d(x, self.filter_cos, padding = self.padding, bias=self.bias)\n",
    "        x_sin = F.conv2d(x, self.filter_sin, padding = self.padding, bias=self.bias)\n",
    "        return torch.cat((x_cos, x_sin), 1)\n",
    "\n",
    "    def generate_parameters(self, dim_out):\n",
    "        torch.manual_seed(1)\n",
    "        # Adjusted to initialize parameters more appropriately for Gabor filters\n",
    "        sigma = nn.Parameter(torch.rand(dim_out, 1) * 2.0 + 0.5) # Random values between 0.5 and 2.5\n",
    "        theta = nn.Parameter(torch.rand(dim_out, 1) * np.pi) # Random values between 0 and Ï€\n",
    "        Lambda = nn.Parameter(torch.rand(dim_out, 1) * 3.0 + 1.0) # Random values between 1.0 and 4.0, how Lambda is good for the detection?\n",
    "        psi = nn.Parameter(torch.rand(dim_out, 1) * 2 * np.pi) # Random values between 0 and 2Ï€\n",
    "        gamma = nn.Parameter(torch.rand(dim_out, 1) * 2.0 + 0.5) # Random values between 0.5 and 2.5\n",
    "        bias = nn.Parameter(torch.randn(dim_out)) # to avoid division by zero\n",
    "        return sigma, theta, Lambda, psi, gamma, bias\n",
    "\n",
    "\n",
    "    def whole_filter(self, cos=True):\n",
    "        # Creating a tensor to hold the Gabor filters for all orientations and scales\n",
    "        result = torch.zeros(self.num_orientations*self.num_scales, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        for i in range(self.num_orientations):\n",
    "            for j in range(self.num_scales):\n",
    "                index = i * self.num_scales + j\n",
    "                # Adjusting parameters for scale and orientation\n",
    "                sigma = self.sigma[index] * (2.1 ** j) # Adjusting sigma for scale\n",
    "                theta = self.theta[index] + i * 2 * np.pi / self.num_orientations # Adjusting theta for orientation\n",
    "                Lambda = self.Lambda[index] # Keeping Lambda constant\n",
    "                psi = self.psi[index] # Keeping psi constant\n",
    "                gamma = self.gamma[index] # Keeping gamma constant\n",
    "                # Generating the Gabor filter for each channel\n",
    "                for k in range(self.in_channels):\n",
    "                    result[index, k] = self.gabor_fn(sigma, theta, Lambda, psi, gamma, self.kernel_size, cos)\n",
    "        return nn.Parameter(result)\n",
    "\n",
    "    def gabor_fn(self, sigma, theta, Lambda, psi, gamma, kernel_size, cos=True):\n",
    "        n = kernel_size // 2\n",
    "        y, x = np.ogrid[-n:n+1, -n:n+1]\n",
    "        y = torch.FloatTensor(y)\n",
    "        x = torch.FloatTensor(x)\n",
    "\n",
    "        x_theta = x * torch.cos(theta) + y * torch.sin(theta)\n",
    "        y_theta = -x * torch.sin(theta) + y * torch.cos(theta)\n",
    "\n",
    "        if cos:\n",
    "            gb = torch.exp(-.5 * (x_theta ** 2 / sigma ** 2 + y_theta ** 2 / sigma ** 2 / gamma ** 2)) * torch.cos(2 * np.pi / Lambda * x_theta + psi)\n",
    "        else:\n",
    "            gb = torch.exp(-.5 * (x_theta ** 2 / sigma ** 2 + y_theta ** 2 / sigma ** 2 / gamma ** 2)) * torch.sin(2 * np.pi / Lambda * x_theta + psi)\n",
    "        \n",
    "        return gb\n",
    "\n",
    "\n",
    "[Python code for data preparation]\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision import transforms\n",
    "import random\n",
    "random.seed(42)\n",
    "# from scipy.ndimage import binary_dilation\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class CenterlineDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, transform=None, augmentation=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        # self.dilated_label_dir = dilated_label_dir\n",
    "        self.transform = transform\n",
    "        # self.save_dilated = save_dilated\n",
    "        self.augmentation = augmentation\n",
    "        # self.dilation_iterations = dilation_iterations\n",
    "\n",
    "        self.images = sorted([img for img in os.listdir(img_dir) if img.endswith('.png')])\n",
    "        self.labels = sorted([label for label in os.listdir(label_dir) if label.endswith('.png')])\n",
    "        print(f\"Found {len(self.images)} images and labels in the dataset.\")\n",
    "\n",
    "        # if add_poor_quality_training:\n",
    "        #     self.poor_quality_img_dir = \"data_poor_quality/samples\"\n",
    "        #     self.poor_quality_label_dir = \"data_poor_quality/labels\"\n",
    "\n",
    "        #     self.poor_quality_images = sorted([img for img in os.listdir(self.poor_quality_img_dir) if img.endswith('.png')])\n",
    "        #     self.poor_quality_labels = sorted([label for label in os.listdir(self.poor_quality_label_dir) if label.endswith('.png')])\n",
    "\n",
    "        #     self.images += self.poor_quality_images\n",
    "        #     self.labels += self.poor_quality_labels\n",
    "\n",
    "        #     print(f\"Added {len(self.poor_quality_images)} poor quality images and labels to the dataset.\")\n",
    "        #     print(f\"Total images and labels in the dataset: {len(self.images)}\")\n",
    "\n",
    "        assert len(self.images) == len(self.labels), \"The number of images and labels do not match!\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, index): # I/O Intensive\n",
    "        img_path = os.path.join(self.img_dir, self.images[index])\n",
    "        label_path = os.path.join(self.label_dir, self.labels[index])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path)\n",
    "            # print((np.array(image) > 255).any())\n",
    "            # image = image.convert(\"L\")  # Grayscale conversion for images. It's like we do autoscaling here.\n",
    "            # print the pixel intensity\n",
    "            # print(np.array(image))\n",
    "            # print(image.mode)\n",
    "\n",
    "            # before convert label to grayscale, make all pixels greater than 0 to 65535\n",
    "            label = Image.open(label_path)\n",
    "            # label_np = np.array(label)\n",
    "            # label_np[label_np > 0] = 65535\n",
    "            # label = Image.fromarray(label_np.astype(np.uint16))\n",
    "            # # assert all pixels are in range 0-65535\n",
    "            # assert (label_np <= 65535).all(), f\"Some pixels are greater than 65535, {label_path}\"\n",
    "            # assert (label_np >= 0).all(), f\"Some pixels are less than 0, {label_path}\"\n",
    "            \n",
    "            label_np = np.array(label)\n",
    "            label_np = (label_np > 0).astype(np.uint8) * 255  # Ensure binary labels, but converts to L mode\n",
    "            # label_np = label_np > 0\n",
    "            # # print(\"The data type of the label np is*********: \", label_np.dtype)\n",
    "            label = Image.fromarray(label_np)\n",
    "            # print(label.mode)\n",
    "\n",
    "\n",
    "            # label = label.convert(\"L\")  # Grayscale conversion for labels\n",
    "            # print(np.array(label))\n",
    "\n",
    "\n",
    "            # Crop the first row and column off, adjusting the size to 64x64\n",
    "            # Assuming the original size is 65x65, crop to get (1, 1, 65, 65)\n",
    "            image = image.crop((1, 1, 65, 65))\n",
    "            label = label.crop((1, 1, 65, 65))\n",
    "\n",
    "            # Processing the label to be binary\n",
    "            # print(np.array(label))\n",
    "            # label_np = np.array(label) > 0  # Ensuring binary values, shape: (64, 64), True or False\n",
    "            # label = Image.fromarray(label_np.astype(np.uint8))  # Convert back to image\n",
    "            # print(np.array(label))\n",
    "\n",
    "            # Dilate the label \n",
    "            # label_np = np.array(label) > 0 \n",
    "            # dilated_label_np = binary_dilation(label_np, iterations=self.dilation_iterations).astype(np.uint8)\n",
    "            # dilated_label = Image.fromarray(dilated_label_np) \n",
    "\n",
    "            # save the dilated label for debugging\n",
    "            # if self.save_dilated:\n",
    "            #     if not os.path.exists(self.dilated_label_dir):\n",
    "            #         os.makedirs(self.dilated_label_dir)\n",
    "            #     dilated_label_path = os.path.join(self.dilated_label_dir, f\"dilated_label_{index}.png\")\n",
    "            #     dilated_label.save(dilated_label_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error opening image or label at index {index}: {e}\")\n",
    "            return None, None, None\n",
    "\n",
    "        # Always apply basic transformations if provided, transformed to a tensor\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "            label = transforms.ToTensor()(label) # ToTensor here works well for 8 bit\n",
    "            # print(torch.max(label), torch.min(label))\n",
    "            # label = self.transform(label)\n",
    "            # dilated_label = self.transform(dilated_label)\n",
    "            \n",
    "        # image = torch.from_numpy(np.array(image, dtype=np.float32))\n",
    "        # label = torch.from_numpy(np.array(label, dtype=np.float32))\n",
    "        \n",
    "        # if image.dim() == 2:\n",
    "        #     image = image.unsqueeze(0)\n",
    "        # if label.dim() == 2:\n",
    "        #     label = label.unsqueeze(0)\n",
    "\n",
    "        # image_min = torch.min(image)\n",
    "        # # print(\"The minimum pixel intensity of the image is: \", image_min)\n",
    "        # image_max = torch.max(image)\n",
    "        # # print(\"The maximum pixel intensity of the image is: \", image_max)\n",
    "        # image = (image - image_min) / (image_max - image_min)\n",
    "        # # print(\"The data type of the image is: \", image.dtype)\n",
    "        \n",
    "        # label_min = torch.min(label)\n",
    "        # # print(\"The minimum pixel intensity of the label is: \", label_min)\n",
    "        # label_max = torch.max(label)\n",
    "        # # print(\"The maximum pixel intensity of the label is: \", label_max)\n",
    "        # label = (label - label_min) / (label_max - label_min)\n",
    "        # # print(\"The data type of the label is: \", label.dtype)\n",
    "                \n",
    "        \n",
    "                \n",
    "        \n",
    "\n",
    "        if self.augmentation:\n",
    "            if self.augmentation == \"rotate\":\n",
    "                angle = random.uniform(-180, 180)\n",
    "                image = TF.rotate(image, angle)\n",
    "                label = TF.rotate(label, angle)\n",
    "                # dilated_label = TF.rotate(dilated_label, angle)\n",
    "            if self.augmentation == \"flip\":\n",
    "                if random.random() > 0.5: # Horizontal flip\n",
    "                    image = TF.hflip(image)\n",
    "                    label = TF.hflip(label)\n",
    "                    # dilated_label = TF.hflip(dilated_label)\n",
    "                # if random.random() > 0.5: # Vertical flip\n",
    "                else:\n",
    "                    image = TF.vflip(image)\n",
    "                    label = TF.vflip(label)\n",
    "                    # dilated_label = TF.vflip(dilated_label)\n",
    "        \n",
    "\n",
    "        return image, label\n",
    "\n",
    "\n",
    "[Python code for inference, image reconstruction because we cut a big image into small patches and do inference on them]\n",
    "\n",
    "\n",
    "\n",
    "class Normalize16BitRange(transforms.ToTensor):\n",
    "    def __call__(self, pic):\n",
    "        '''\n",
    "            Input: PIL Image or numpy array\n",
    "            Output: Tensor Normalized to [0, 1] based on the actual data range\n",
    "        '''\n",
    "        img = torch.from_numpy(np.array(pic, dtype=np.float32, copy=True))\n",
    "        \n",
    "        # mimic ToTensor having shape [C, H, W]\n",
    "        if img.dim() == 2:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Normalization based on the actual data range\n",
    "        # min_val = torch.min(img)\n",
    "        # max_val = torch.max(img)\n",
    "        # min_val = 108\n",
    "        max_val = 2000\n",
    "        # pixels greater than 2826 fixed at 2826\n",
    "        img = torch.where(img > max_val, torch.tensor(max_val), img)\n",
    "        # pixels less than 90 fixed at 90\n",
    "        # img = torch.where(img < min_val, torch.tensor(min_val), img)\n",
    "        img = img / max_val  # Normalize to [0, 1]\n",
    "\n",
    "        return img\n",
    "\n",
    "\n",
    "# Patch The Input Image\n",
    "\n",
    "\n",
    "def image_to_patches(image, patch_size=64, overlap=32):\n",
    "\n",
    "    #- Splits the image into patches, with optional overlap.\n",
    "    #- Input: PIL Image, uint16 \n",
    "    #- Output: Numpy array of patches and their center coordinates\n",
    "\n",
    "    patch_tuples = []\n",
    "    patches = []\n",
    "    img_np = np.array(image)\n",
    "    stride = patch_size - overlap\n",
    "    index = 0\n",
    "    for y in range(0, img_np.shape[0], stride):\n",
    "        for x in range(0, img_np.shape[1], stride):\n",
    "            center_x = x + patch_size // 2\n",
    "            center_y = y + patch_size // 2\n",
    "            \n",
    "            if y + patch_size <= img_np.shape[0] and x + patch_size <= img_np.shape[1]:\n",
    "                patch = img_np[y:y+patch_size, x:x+patch_size]\n",
    "                patches.append(patch)\n",
    "                patch_tuples.append((index, (center_x, center_y), patch))\n",
    "            else:  # Handle edges by padding\n",
    "                patch = np.pad(img_np[y:min(y+patch_size, img_np.shape[0]), x:min(x+patch_size, img_np.shape[1])],\n",
    "                               ((0, patch_size - (min(y + patch_size, img_np.shape[0]) - y)),\n",
    "                                (0, patch_size - (min(x + patch_size, img_np.shape[1]) - x))),\n",
    "                               mode='constant', constant_values=0)\n",
    "                patches.append(patch)\n",
    "                patch_tuples.append((index, (center_x, center_y), patch))\n",
    "            index += 1\n",
    "    return np.array(patches), patch_tuples\n",
    "\n",
    "\n",
    "def model_predict_patches_raw(patches, model):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    for patch in patches:\n",
    "        patch_tensor = Normalize16BitRange()(patch).unsqueeze(0)\n",
    "        patch_tensor = patch_tensor.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(patch_tensor)\n",
    "        pred = pred.cpu().numpy().squeeze(0).squeeze(0)\n",
    "        predictions.append(pred)\n",
    "    return np.array(predictions)\n",
    " \n",
    "\n",
    "def patches_to_image_raw(patches, image_size, patch_size=64, overlap=32):\n",
    "    stride = patch_size - overlap\n",
    "    reconstructed_image = np.zeros(image_size)\n",
    "    count_matrix = np.zeros(image_size)\n",
    "    patch_idx = 0\n",
    "\n",
    "    for y in range(0, image_size[0], stride):\n",
    "        for x in range(0, image_size[1], stride):\n",
    "            # Calculate boundaries for patch application\n",
    "            end_y = min(y + patch_size, image_size[0])\n",
    "            end_x = min(x + patch_size, image_size[1])\n",
    "            # Calculate the actual height and width to be used from the patch\n",
    "            patch_height = end_y - y\n",
    "            patch_width = end_x - x\n",
    "            reconstructed_image[y:end_y, x:end_x] += patches[patch_idx][:patch_height, :patch_width]\n",
    "            count_matrix[y:end_y, x:end_x] += 1\n",
    "            patch_idx += 1\n",
    "    # Avoid division by zero\n",
    "    count_matrix[count_matrix == 0] = 1\n",
    "    reconstructed_image /= count_matrix\n",
    "\n",
    "    reconstructed_image[reconstructed_image > 0.5] = 255\n",
    "    reconstructed_image[reconstructed_image <= 0.5] = 0\n",
    "    show_image(reconstructed_image)\n",
    "    return reconstructed_image\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centerline Detection of Neuronal Dendrites: Deep Learning Model Workflow\n",
      "\n",
      "Authors: [Your Names]\n",
      "Date: [Current Date]\n",
      "\n",
      "1. Introduction\n",
      "This project focuses on the development of a deep learning model for centerline detection of Neuronal Dendrite membranes. The primary goal is to predict the 'backbone' of the structure with outputs that are as continuous as possible. This task falls under the category of binary segmentation, focusing on identifying the central lines within membrane structures in grayscale images.\n",
      "\n",
      "The dataset consists of 16-bit grayscale images of size 65x65 pixels. The membrane structures have intensity values ranging from approximately 150 to 1000, whereas the background intensity ranges from approximately 110 to 150. However, it is noted that in some areas, the noise pixel values are similar to those of the membranes, which presents an additional challenge for accurate segmentation.\n",
      "\n",
      "2. Data Preparation\n",
      "2.1 Data Collection\n",
      "The data used for this task was provided by evaluators, consisting of 28,634 samples stored as 65x65 16-bit PNG images. Each image is grayscale, with corresponding segmentation masks also stored as 65x65 PNGs, ensuring a direct mapping between images and their labels.\n",
      "\n",
      "2.2 Data Preprocessing\n",
      "Effective data preprocessing is critical for preparing the data for model training. The following steps were taken:\n",
      "\n",
      "1. Intensity Range Adjustment:\n",
      "- Adjusted the intensity range for the background from [110, 150] to [100, 150].\n",
      "- Adjusted the intensity range for the membrane from [150, 1000] to [150, 1010].\n",
      "\n",
      "2. Handling Empty Samples:\n",
      "- Reduced 1,000 samples (3.5% of the total dataset) with no centerline information to 100.\n",
      "\n",
      "3. Normalization and Clipping:\n",
      "- Clipped images to the specified ranges and normalized to the [0, 1] range using MINMAX Normalization.\n",
      "\n",
      "4. Data Augmentation:\n",
      "- Applied elastic deformation, horizontal and vertical flips, and a Patch and Reconstruct approach.\n",
      "\n",
      "5. Dataset and DataLoader Construction:\n",
      "- Implemented custom PyTorch Dataset classes (CenterlineDataset and PatchConstructDataset) to handle preprocessing steps and facilitate loading of data during training.\n",
      "\n",
      "2.3 Data Splitting\n",
      "The dataset was split into training, validation, and test sets with a ratio of 70% training, 15% validation, and 15% testing.\n",
      "\n",
      "3. Model Development\n",
      "3.1 Model Selection\n",
      "The U-Net architecture was selected due to its effectiveness in biomedical image segmentation tasks. Several variations of U-Net were experimented with to enhance performance.\n",
      "\n",
      "3.2 Model Architecture\n",
      "1. Standard U-Net: A custom baseline model with 19,312,001 parameters.\n",
      "2. Mini U-Net: A smaller version with 4,713,601 parameters.\n",
      "3. Extended U-Net: An extended version with additional layers and 19,335,77 parameters.\n",
      "4. U-Net with Self-Attention (U-Net SA): Incorporates self-attention mechanisms with 19,583,604 parameters.\n",
      "5. Mini U-Net with Self-Attention (Mini U-Net SA): A smaller version of U-Net SA with 4,778,83 parameters.\n",
      "\n",
      "3.3 Training\n",
      "3.3.1 Training Setup\n",
      "1. Hyperparameters:\n",
      "- Learning Rate: Initially set to 1e-3 and adjusted towards 1e-4.\n",
      "- Batch Size: 128 (64 on local GPU).\n",
      "- Number of Epochs: Set to 50 epochs or until convergence.\n",
      "\n",
      "2. Loss Function:\n",
      "- Binary Cross-Entropy Loss.\n",
      "- Combined Dice Loss and BCE Loss.\n",
      "\n",
      "3. Optimization Algorithm:\n",
      "- Adam Optimizer.\n",
      "\n",
      "3.4 Testing\n",
      "3.4.1 Model Evaluation\n",
      "1. Performance Metrics:\n",
      "- Intersection over Union (IoU).\n",
      "- Dice Coefficient.\n",
      "- Precision and Recall.\n",
      "- F1-Score.\n",
      "\n",
      "2. Evaluation Procedure:\n",
      "- Loading the Test Data.\n",
      "- Model Inference.\n",
      "- Metric Calculation.\n",
      "- Visualization.\n",
      "\n",
      "4. Inference\n",
      "4.1 Loading the Trained Model\n",
      "1. Model Loading:\n",
      "- Load the trained model weights from the saved checkpoint.\n",
      "\n",
      "4.2 Running Inference on New Data\n",
      "1. Data Preparation:\n",
      "- Loading Images.\n",
      "- Clipping Intensity Values.\n",
      "- Normalization.\n",
      "- Adding Channel Dimension.\n",
      "\n",
      "2. Inference Process:\n",
      "- Pass the new data through the loaded model.\n",
      "\n",
      "3. Post-processing:\n",
      "- Thresholding the output probabilities to obtain binary masks.\n",
      "- Applying morphological operations depending on the task requirement.\n",
      "\n",
      "4.3 Visualization of Inferred Segmentation Masks\n",
      "1. Displaying Results:\n",
      "- Visualize the original images alongside the predicted segmentation masks.\n",
      "\n",
      "2. Saving Results:\n",
      "- Save the predicted masks for further analysis or for use in downstream applications.\n",
      "\n",
      "5. Results\n",
      "The models were evaluated based on several metrics, with a particular focus on their balance between precision and recall. The Normal U-Net with BCE Loss achieved the highest F1 score, indicating the best overall performance in balancing precision and recall. Other models also performed well, each with its strengths, suggesting various trade-offs between complexity, precision, and recall.\n"
     ]
    }
   ],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mortal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
